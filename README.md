# Natural-Language-Processing-II-

Various concepts and coding excercise related to NLP.

# Content

- 0- Implementation of Basic LSTM and concepts.
- 1- Topic Modeling using SVD, truncated SVD, NMF.
- 2- Randomized SVD.
- 3- Sentiment Classification using bagofwords model.
- 4- Movie Sentiment Analysis based on review using GRU.
- 5)- Language Translation using Attention model.
- 6)- Text Generation using fine-tine GPT-2 model.
- 7)- Text classification using BERT .
- 8)- Emotion_Analysis.
- 9)- Word Embedding.
- 10)- Part of Speech(POS) Tagging.
- 11)- Named Entity Recognition (NER).
- 12)- CMU(Carnegie Mellon University) NLP lectures.
- 13)- NLP application for Legal contract documents.
- 14)- Information extraction from large documents such as NIPS dataset.
- 15)- Text Extraction from unstructured data using regular expression.


# Modules
For data processing : pandas, numpy, eli5 <br>
For visualization : matplotlib, seaborn , plotly, TabPy <br>
For machine learning : sklearn, SciPy <br>
Web Scrapping : Beautifulsoup, Urllib , Scrapy <br>
For text mining : spacy, nltk,re, TextBlob, Gensim <br>
For deep learning : pytorch, tensorflow, keras, fastai, Dynet(Dynamic Neural Network) <br>

# References

- For PyTorch: https://pytorch.org/tutorials/
- Gensim: https://pypi.org/project/gensim/
- Text BLob: https://textblob.readthedocs.io/en/dev/
- NLTK : https://www.nltk.org/
- Spacy: https://nlpforhackers.io/complete-guide-to-spacy/
- Spacy Documentation:https://spacy.io/usage/linguistic-features
- Explosion:https://github.com/koaning/spacy-youtube-material
- Keras: https://keras.io/
- Attention Model: Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Advances in neural information processing systems (pp. 3104-3112).
- Attention Model: Cho, K., Van MerriÃ«nboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.
- Carnegie Mellon University NLP : http://www.cs.cmu.edu/~nasmith/nlp-cl.html
- Carnegie Mellon University NLP: http://demo.clab.cs.cmu.edu/NLP/
- seq2seq Model with Attention: https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/
- For Transformer: The Illustrated Transformer
- For GPT-2 : OpenAI blog post on GPT-2
- Silicon Valley Python Workshops: https://github.com/PythonWorkshop/intro-to-nlp-with-pytorch
- Elvis Saravia Talk:https://github.com/omarsar/nlp_pycon
- For DyNet: https://github.com/clab/dynet
- Named Entity Recognition (NER) : https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da

# Credit:

Special Thanks to PyTorch Tutorials, fastai lectures, scoutbee lectures,PyCon NLP 2019, Silicon Valley Python Workshops Microsoft Reactor event,CMU Neural Nets for NLP 2019, which helped me immensely for learning key concepts of NLP & coding in general, Spacy documentation
