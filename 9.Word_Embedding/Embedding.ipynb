{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Embedding.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMr3zF1czINE",
        "colab_type": "text"
      },
      "source": [
        "# Embeddings\n",
        "\n",
        "An embedding maps discrete, categorical values to a continous space. Major advances in NLP applications have come from these continuous representations of words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NegCcBRizK9K",
        "colab_type": "text"
      },
      "source": [
        "# 1)- Importing key Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vKk3h5XzJtK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#support both Python 2 and Python 3 with minimal overhead.\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# I am an engineer. I care only about error not warning. So, let's be maverick and ignore warnings.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uz-sdBYzdWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For data processing and maths\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import tqdm\n",
        "#For Visuals\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "from matplotlib import rcParams\n",
        "rcParams['figure.figsize'] = 11, 8\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfA3Jd9qzRFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For pyTorch\n",
        "import torch\n",
        "from torch.nn.functional import one_hot\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KDWkPy6ztth",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a4d2dfab-c8b9-49eb-f320-947deffed73f"
      },
      "source": [
        "! pip install version_information"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: version_information in /usr/local/lib/python3.6/dist-packages (1.0.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTzjTOn9zt1h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "32e4adca-43dd-49b4-fa21-d37ac264beca"
      },
      "source": [
        "# first install: pip install version_information\n",
        "%reload_ext version_information\n",
        "%version_information pandas,torch,numpy,seaborn, matplotlib"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "\\begin{tabular}{|l|l|}\\hline\n{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\nPython & 3.6.8 64bit [GCC 8.0.1 20180414 (experimental) [trunk revision 259383] \\\\ \\hline\nIPython & 5.5.0 \\\\ \\hline\nOS & Linux 4.14.137+ x86\\_64 with Ubuntu 18.04 bionic \\\\ \\hline\npandas & 0.24.2 \\\\ \\hline\ntorch & 1.3.0+cu100 \\\\ \\hline\nnumpy & 1.16.5 \\\\ \\hline\nseaborn & 0.9.0 \\\\ \\hline\nmatplotlib & 3.0.3 \\\\ \\hline\n\\hline \\multicolumn{2}{|l|}{Mon Oct 21 10:59:51 2019 UTC} \\\\ \\hline\n\\end{tabular}\n",
            "application/json": {
              "Software versions": [
                {
                  "version": "3.6.8 64bit [GCC 8.0.1 20180414 (experimental) [trunk revision 259383]",
                  "module": "Python"
                },
                {
                  "version": "5.5.0",
                  "module": "IPython"
                },
                {
                  "version": "Linux 4.14.137+ x86_64 with Ubuntu 18.04 bionic",
                  "module": "OS"
                },
                {
                  "version": "0.24.2",
                  "module": "pandas"
                },
                {
                  "version": "1.3.0+cu100",
                  "module": "torch"
                },
                {
                  "version": "1.16.5",
                  "module": "numpy"
                },
                {
                  "version": "0.9.0",
                  "module": "seaborn"
                },
                {
                  "version": "3.0.3",
                  "module": "matplotlib"
                }
              ]
            },
            "text/html": [
              "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.6.8 64bit [GCC 8.0.1 20180414 (experimental) [trunk revision 259383]</td></tr><tr><td>IPython</td><td>5.5.0</td></tr><tr><td>OS</td><td>Linux 4.14.137+ x86_64 with Ubuntu 18.04 bionic</td></tr><tr><td>pandas</td><td>0.24.2</td></tr><tr><td>torch</td><td>1.3.0+cu100</td></tr><tr><td>numpy</td><td>1.16.5</td></tr><tr><td>seaborn</td><td>0.9.0</td></tr><tr><td>matplotlib</td><td>3.0.3</td></tr><tr><td colspan='2'>Mon Oct 21 10:59:51 2019 UTC</td></tr></table>"
            ],
            "text/plain": [
              "Software versions\n",
              "Python 3.6.8 64bit [GCC 8.0.1 20180414 (experimental) [trunk revision 259383]\n",
              "IPython 5.5.0\n",
              "OS Linux 4.14.137+ x86_64 with Ubuntu 18.04 bionic\n",
              "pandas 0.24.2\n",
              "torch 1.3.0+cu100\n",
              "numpy 1.16.5\n",
              "seaborn 0.9.0\n",
              "matplotlib 3.0.3\n",
              "Mon Oct 21 10:59:51 2019 UTC"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obRGsgKpzqkq",
        "colab_type": "text"
      },
      "source": [
        "# 2)- One-Hot Encoding\n",
        "\n",
        "example 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG4nKfmdzRNy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "61277d04-090c-42f8-a4d3-c6e73654525e"
      },
      "source": [
        "sentence = \"the quick brown fox jumped over the lazy dog\"\n",
        "words = sentence.split(' ')\n",
        "print(words)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofPACpAiz78F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "09acffce-c293-4c0c-c6ea-969cb8b2fb34"
      },
      "source": [
        "vocab1 = list(set(words))\n",
        "print(vocab1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['lazy', 'over', 'brown', 'fox', 'jumped', 'the', 'quick', 'dog']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgPTq1a_z-Np",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fe41d4a3-77ee-4163-869d-95c32d194b6d"
      },
      "source": [
        "# Number of words in our vocabulary\n",
        "len(vocab1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U0DmQ-d0AXA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "db3086d0-997d-4651-c256-da6d91776eca"
      },
      "source": [
        "# Convert words to indexes\n",
        "word_to_ix1 = {word: i for i, word in enumerate(vocab1)}\n",
        "print(word_to_ix1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'lazy': 0, 'over': 1, 'brown': 2, 'fox': 3, 'jumped': 4, 'the': 5, 'quick': 6, 'dog': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReFoRMWT0pum",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9aade9d2-0db5-44d2-aa43-7944a3658d2a"
      },
      "source": [
        "type(word_to_ix1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v5px0zA0rs3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "06adf141-4ece-4d60-dfcd-080d59149e2b"
      },
      "source": [
        "word_to_ix1['over']"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cmfyL8T0vFn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8b5ba1b2-8ad5-48f3-81e8-ce24375d6efa"
      },
      "source": [
        "word_to_ix1['dog']"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X0tTwWr00yd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "56b6b3fb-724d-43eb-ba8b-d384813ccd64"
      },
      "source": [
        "from torch.nn.functional import one_hot\n",
        "\n",
        "words = torch.tensor([word_to_ix1[w] for w in vocab1], dtype=torch.long)\n",
        "\n",
        "one_hot_encoding = one_hot(words)\n",
        "print(vocab1)\n",
        "print(one_hot_encoding)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['lazy', 'over', 'brown', 'fox', 'jumped', 'the', 'quick', 'dog']\n",
            "tensor([[1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 1, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 1, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 1]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7cjbfmT1JXw",
        "colab_type": "text"
      },
      "source": [
        "There are alot of zeros and if we have 10000 words then our voacb will be high. Suppose 1000 and then we will have 1000 vectors for one-hot encoding. It is not a wise approach. So, we do vectorize our word columns using word embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs9HPy-f1Cn5",
        "colab_type": "text"
      },
      "source": [
        "# 3)-Word Embedding\n",
        "\n",
        "Here we will predict next word having first two words in sequence\n",
        "\n",
        "- Predict the probability of a word based on the words around it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pahlACV0-D9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Context is the number of words we are using as a context for the next word we want to predict\n",
        "CONTEXT_SIZE = 2\n",
        "\n",
        "# Embedding dimension is the size of the embedding vector\n",
        "EMBEDDING_DIM = 10\n",
        "\n",
        "# Size of the hidden layer\n",
        "HIDDEN_DIM = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGh3Yvez1e30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sentence = \"\"\"EV battery pack with battery cooling assembly and method:\n",
        "An electric vehicle battery pack includes an array of battery cells each cell having an upper cell surface and a lower cell surface, the lower cell surface having a positive and a negative terminal; \n",
        "and a thermal assembly in thermally-conductive contact with the upper cell surfaces of the array. A battery pack cooling method is also disclosed.\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9eAoSj81wU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sentence=test_sentence.lower().split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1cENYnY12VT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b87f4384-60b9-4953-f58f-b8221c94ee72"
      },
      "source": [
        "test_sentence[:5]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ev', 'battery', 'pack', 'with', 'battery']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM6anNFG13_2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9a4c45fa-cf8f-4e62-a224-e2cf934d45c0"
      },
      "source": [
        "# Build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
        "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
        "            for i in range(len(test_sentence) - 2)]\n",
        "# print the first 3, just so you can see what they look like\n",
        "print(trigrams[:5])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(['ev', 'battery'], 'pack'), (['battery', 'pack'], 'with'), (['pack', 'with'], 'battery'), (['with', 'battery'], 'cooling'), (['battery', 'cooling'], 'assembly')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVbdVHqc2CwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab2 = list(set(test_sentence))\n",
        "word_to_ix2 = {word: i for i, word in enumerate(vocab2)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQU_7-LX2dcG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cc82fa03-0276-4bb3-87b5-2870375b191d"
      },
      "source": [
        "len(vocab2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYNIkHtr2fcm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "outputId": "b99a4fe5-4e62-4013-a953-85b7638dff10"
      },
      "source": [
        "word_to_ix2"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 11,\n",
              " 'also': 13,\n",
              " 'an': 27,\n",
              " 'and': 25,\n",
              " 'array': 35,\n",
              " 'array.': 6,\n",
              " 'assembly': 14,\n",
              " 'battery': 23,\n",
              " 'cell': 18,\n",
              " 'cells': 34,\n",
              " 'contact': 15,\n",
              " 'cooling': 30,\n",
              " 'disclosed.': 1,\n",
              " 'each': 29,\n",
              " 'electric': 8,\n",
              " 'ev': 24,\n",
              " 'having': 7,\n",
              " 'in': 10,\n",
              " 'includes': 2,\n",
              " 'is': 33,\n",
              " 'lower': 12,\n",
              " 'method': 28,\n",
              " 'method:': 9,\n",
              " 'negative': 31,\n",
              " 'of': 21,\n",
              " 'pack': 36,\n",
              " 'positive': 32,\n",
              " 'surface': 26,\n",
              " 'surface,': 0,\n",
              " 'surfaces': 16,\n",
              " 'terminal;': 17,\n",
              " 'the': 19,\n",
              " 'thermal': 4,\n",
              " 'thermally-conductive': 20,\n",
              " 'upper': 22,\n",
              " 'vehicle': 5,\n",
              " 'with': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLRuEmgj2jdv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "519b748b-dc13-47b9-8827-6ae60233155b"
      },
      "source": [
        "word_to_ix2['method']"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYca071Q24pW",
        "colab_type": "text"
      },
      "source": [
        "# 4)-Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUc2OVqf2tLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramLanguageModeler(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(NGramLanguageModeler, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, HIDDEN_DIM)\n",
        "        self.linear2 = nn.Linear(HIDDEN_DIM, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1))\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujwjJPW028ka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.001\n",
        "losses = []\n",
        "loss_function = nn.NLLLoss()  # negative log likelihood\n",
        "model = NGramLanguageModeler(len(vocab2), EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcHyrGGU3C0t",
        "colab_type": "text"
      },
      "source": [
        "**process with steps**\n",
        "\n",
        "https://github.com/PythonWorkshop/intro-to-nlp-with-pytorch/blob/master/Word%20Embeddings/Word%20Embeddings.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c58SNt_g2-7B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "a920a481-e4ad-4c3a-c53e-820698df7646"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for epoch in range(25):\n",
        "    total_loss = 0\n",
        "\n",
        "    iterator = tqdm(trigrams)\n",
        "    for context, target in iterator:\n",
        "        # (['When', 'forty'], 'winters')\n",
        "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
        "        # into integer indices and wrap them in tensors)\n",
        "        context_idxs = torch.tensor([word_to_ix2[w] for w in context], dtype=torch.long)\n",
        "\n",
        "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
        "        # new instance, you need to zero out the gradients from the old\n",
        "        # instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 3. Run the forward pass, getting log probabilities over next\n",
        "        # words\n",
        "        log_probs = model(context_idxs)\n",
        "\n",
        "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
        "        # word wrapped in a tensor)\n",
        "        loss = loss_function(log_probs, torch.tensor([word_to_ix2[target]], dtype=torch.long))\n",
        "\n",
        "        # Step 5. Do the backward pass and update the gradient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "        total_loss += loss.item()\n",
        "        iterator.set_postfix(loss=float(loss))\n",
        "    losses.append(total_loss)\n",
        "    # add progress bar with epochs"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 282.90it/s, loss=4.07]\n",
            "100%|██████████| 64/64 [00:00<00:00, 286.89it/s, loss=4.03]\n",
            "100%|██████████| 64/64 [00:00<00:00, 317.07it/s, loss=4]\n",
            "100%|██████████| 64/64 [00:00<00:00, 317.24it/s, loss=3.96]\n",
            "100%|██████████| 64/64 [00:00<00:00, 309.69it/s, loss=3.92]\n",
            "100%|██████████| 64/64 [00:00<00:00, 316.05it/s, loss=3.88]\n",
            "100%|██████████| 64/64 [00:00<00:00, 331.95it/s, loss=3.84]\n",
            "100%|██████████| 64/64 [00:00<00:00, 320.33it/s, loss=3.81]\n",
            "100%|██████████| 64/64 [00:00<00:00, 317.03it/s, loss=3.77]\n",
            "100%|██████████| 64/64 [00:00<00:00, 330.28it/s, loss=3.73]\n",
            "100%|██████████| 64/64 [00:00<00:00, 316.57it/s, loss=3.69]\n",
            "100%|██████████| 64/64 [00:00<00:00, 287.85it/s, loss=3.66]\n",
            "100%|██████████| 64/64 [00:00<00:00, 307.46it/s, loss=3.62]\n",
            "100%|██████████| 64/64 [00:00<00:00, 315.98it/s, loss=3.58]\n",
            "100%|██████████| 64/64 [00:00<00:00, 324.03it/s, loss=3.54]\n",
            "100%|██████████| 64/64 [00:00<00:00, 323.50it/s, loss=3.51]\n",
            "100%|██████████| 64/64 [00:00<00:00, 334.45it/s, loss=3.47]\n",
            "100%|██████████| 64/64 [00:00<00:00, 322.63it/s, loss=3.43]\n",
            "100%|██████████| 64/64 [00:00<00:00, 332.01it/s, loss=3.39]\n",
            "100%|██████████| 64/64 [00:00<00:00, 319.46it/s, loss=3.35]\n",
            "100%|██████████| 64/64 [00:00<00:00, 327.37it/s, loss=3.31]\n",
            "100%|██████████| 64/64 [00:00<00:00, 326.30it/s, loss=3.28]\n",
            "100%|██████████| 64/64 [00:00<00:00, 319.87it/s, loss=3.24]\n",
            "100%|██████████| 64/64 [00:00<00:00, 364.33it/s, loss=3.2]\n",
            "100%|██████████| 64/64 [00:00<00:00, 400.72it/s, loss=3.16]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B58ojGc13lXR",
        "colab_type": "text"
      },
      "source": [
        "**Check the structure of our model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va_eceFc3OaU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "b3900af3-9d2e-428a-9623-2299cd20c619"
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NGramLanguageModeler(\n",
              "  (embeddings): Embedding(37, 10)\n",
              "  (linear1): Linear(in_features=20, out_features=256, bias=True)\n",
              "  (linear2): Linear(in_features=256, out_features=37, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3iek9Ut3weP",
        "colab_type": "text"
      },
      "source": [
        "**Finally checking output**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7Oz6cLm3pZH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "bc52d341-927b-4e10-c8e4-22f0ca8310e3"
      },
      "source": [
        "import numpy\n",
        "\n",
        "with torch.no_grad():\n",
        "    context = ['ev', 'battery']\n",
        "    context_idxs = torch.tensor([word_to_ix2[w] for w in context], dtype=torch.long)\n",
        "    pred = model(context_idxs)\n",
        "    print(pred)\n",
        "    index_of_prediction = numpy.argmax(pred)\n",
        "    print(vocab2[index_of_prediction])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-3.8903, -3.8992, -4.0499, -3.5640, -4.1671, -3.5255, -3.6053, -3.6632,\n",
            "         -3.9569, -4.0884, -3.8815, -3.1810, -3.8464, -4.0280, -3.6784, -3.7553,\n",
            "         -3.5279, -4.3367, -3.3683, -3.3198, -3.8685, -3.3507, -3.6641, -3.3333,\n",
            "         -4.3641, -2.7671, -3.6448, -3.7213, -4.0266, -3.7237, -3.2755, -4.0852,\n",
            "         -3.9153, -3.7027, -3.9384, -3.7946, -2.4018]])\n",
            "pack\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV76AvBH38yQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "09c32a79-77fe-4eef-e99b-a0a4e73c9e07"
      },
      "source": [
        "with torch.no_grad():\n",
        "    context = ['battery', 'cooling']\n",
        "    context_idxs = torch.tensor([word_to_ix2[w] for w in context], dtype=torch.long)\n",
        "    pred = model(context_idxs)\n",
        "    print(pred)\n",
        "    index_of_prediction = numpy.argmax(pred)\n",
        "    print(vocab2[index_of_prediction])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-3.6538, -4.0866, -3.4565, -3.1560, -3.8800, -3.9365, -3.7660, -4.1569,\n",
            "         -3.9080, -4.2016, -4.0053, -3.0585, -3.6159, -3.9280, -3.3109, -3.7641,\n",
            "         -3.9187, -4.0829, -2.6884, -3.3106, -4.1795, -3.6176, -3.6277, -2.7057,\n",
            "         -4.5562, -3.1354, -3.5569, -3.3007, -3.6466, -3.8102, -3.4049, -4.2331,\n",
            "         -4.0830, -4.1373, -4.0375, -3.6279, -3.5538]])\n",
            "cell\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}