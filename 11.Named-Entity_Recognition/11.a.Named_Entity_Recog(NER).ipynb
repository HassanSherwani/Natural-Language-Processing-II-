{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11.a.Named_Entity_Recog(NER).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L5WEerzzSeID"
      },
      "source": [
        "# Named-entity recognition (NER)\n",
        "\n",
        "- using Bi-LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vnDcCpN-Sw9d"
      },
      "source": [
        "# 1)- Importing key modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ormhGRA1SkYP",
        "colab": {}
      },
      "source": [
        "#support both Python 2 and Python 3 with minimal overhead.\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# I am an engineer. I care only about error not warning. So, let's be maverick and ignore warnings.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ffmlRA40S1bS",
        "colab": {}
      },
      "source": [
        "# For data processing and maths\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "#For Visuals\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "from matplotlib import rcParams\n",
        "rcParams['figure.figsize'] = 11, 8\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nybw0TuJT_oO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "60c64caf-362b-411e-8787-dae28c95726e"
      },
      "source": [
        "! pip install jdc"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jdc\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/cb/9afea749985eef20f3160e8826a531c7502e40c35a038dfe49b67726e9a0/jdc-0.0.9-py2.py3-none-any.whl\n",
            "Installing collected packages: jdc\n",
            "Successfully installed jdc-0.0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BJ2K4n6-S_tg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ada700a4-b353-4627-f494-ff9c81bb1273"
      },
      "source": [
        "# for deep learning and neural network\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import jdc\n",
        "import json\n",
        "from collections import defaultdict, OrderedDict\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f7cf72ad170>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rNbXWUUTTLOV",
        "outputId": "a0a76a94-75e9-4b5a-876e-f19bc0500ee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "! pip install version_information"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting version_information\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b0/6088e15b9ac43a08ccd300d68e0b900a20cf62077596c11ad11dd8cc9e4b/version_information-1.0.3.tar.gz\n",
            "Building wheels for collected packages: version-information\n",
            "  Building wheel for version-information (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for version-information: filename=version_information-1.0.3-cp36-none-any.whl size=3880 sha256=a1e58195737ace4f7cbf73752a29ce4dd9416100e1cc5567a222e17be383224c\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/4c/b3/1976ac11dbd802723b564de1acaa453a72c36c95827e576321\n",
            "Successfully built version-information\n",
            "Installing collected packages: version-information\n",
            "Successfully installed version-information-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WRJECjnJTOyF",
        "outputId": "a9903d4d-9cef-47f7-89fa-edadce0b8b3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "# first install: pip install version_information\n",
        "%reload_ext version_information\n",
        "%version_information pandas,torch,numpy,seaborn, matplotlib"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "\\begin{tabular}{|l|l|}\\hline\n{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\nPython & 3.6.8 64bit [GCC 8.3.0] \\\\ \\hline\nIPython & 5.5.0 \\\\ \\hline\nOS & Linux 4.14.137+ x86\\_64 with Ubuntu 18.04 bionic \\\\ \\hline\npandas & 0.24.2 \\\\ \\hline\ntorch & 1.3.0+cu100 \\\\ \\hline\nnumpy & 1.16.5 \\\\ \\hline\nseaborn & 0.9.0 \\\\ \\hline\nmatplotlib & 3.0.3 \\\\ \\hline\n\\hline \\multicolumn{2}{|l|}{Tue Oct 22 16:52:39 2019 UTC} \\\\ \\hline\n\\end{tabular}\n",
            "application/json": {
              "Software versions": [
                {
                  "version": "3.6.8 64bit [GCC 8.3.0]",
                  "module": "Python"
                },
                {
                  "version": "5.5.0",
                  "module": "IPython"
                },
                {
                  "version": "Linux 4.14.137+ x86_64 with Ubuntu 18.04 bionic",
                  "module": "OS"
                },
                {
                  "version": "0.24.2",
                  "module": "pandas"
                },
                {
                  "version": "1.3.0+cu100",
                  "module": "torch"
                },
                {
                  "version": "1.16.5",
                  "module": "numpy"
                },
                {
                  "version": "0.9.0",
                  "module": "seaborn"
                },
                {
                  "version": "3.0.3",
                  "module": "matplotlib"
                }
              ]
            },
            "text/html": [
              "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.6.8 64bit [GCC 8.3.0]</td></tr><tr><td>IPython</td><td>5.5.0</td></tr><tr><td>OS</td><td>Linux 4.14.137+ x86_64 with Ubuntu 18.04 bionic</td></tr><tr><td>pandas</td><td>0.24.2</td></tr><tr><td>torch</td><td>1.3.0+cu100</td></tr><tr><td>numpy</td><td>1.16.5</td></tr><tr><td>seaborn</td><td>0.9.0</td></tr><tr><td>matplotlib</td><td>3.0.3</td></tr><tr><td colspan='2'>Tue Oct 22 16:52:39 2019 UTC</td></tr></table>"
            ],
            "text/plain": [
              "Software versions\n",
              "Python 3.6.8 64bit [GCC 8.3.0]\n",
              "IPython 5.5.0\n",
              "OS Linux 4.14.137+ x86_64 with Ubuntu 18.04 bionic\n",
              "pandas 0.24.2\n",
              "torch 1.3.0+cu100\n",
              "numpy 1.16.5\n",
              "seaborn 0.9.0\n",
              "matplotlib 3.0.3\n",
              "Tue Oct 22 16:52:39 2019 UTC"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYRHSQ0cUHdV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b5274043-a470-40b1-9ec2-5a47c92cf245"
      },
      "source": [
        "# check device if it is gpu or cpu\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5zjfC12iTUFU"
      },
      "source": [
        "# 2)- Example data & its preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6mBhldLUU6M",
        "colab_type": "text"
      },
      "source": [
        "### 2.1)- Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzIBMO9AUXef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def argmax(vec):\n",
        "    \"\"\"Return the argmax as a python int\"\"\"\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        seq - the sequence (array)\n",
        "        to_ix - the indices to which seqence values are converted (dict)\n",
        "\n",
        "    Output:\n",
        "        Numerical tensor\n",
        "        \"\"\"\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "def log_sum_exp(vec):\n",
        "    \"\"\"Compute log sum exp in a numerically stable way for \n",
        "    the forward algorithm\"\"\"\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4reVdZzvYG9e",
        "colab_type": "text"
      },
      "source": [
        "### 2.2)- Getting doccano\n",
        "\n",
        "https://github.com/chakki-works/doccano#installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG43kymUVgZI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "24947ec1-6da2-4911-e49a-458ad3940315"
      },
      "source": [
        "#! pip install psycopg2 "
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: psycopg2 in /usr/local/lib/python3.6/dist-packages (2.7.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHCdTKpPVvLX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "74015cfb-fc7c-41e3-a9a5-47ad90297f6c"
      },
      "source": [
        "#! git clone https://github.com/chakki-works/doccano.git\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'doccano'...\n",
            "remote: Enumerating objects: 168, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/168)\u001b[K\rremote: Counting objects:   1% (2/168)\u001b[K\rremote: Counting objects:   2% (4/168)\u001b[K\rremote: Counting objects:   3% (6/168)\u001b[K\rremote: Counting objects:   4% (7/168)\u001b[K\rremote: Counting objects:   5% (9/168)\u001b[K\rremote: Counting objects:   6% (11/168)\u001b[K\rremote: Counting objects:   7% (12/168)\u001b[K\rremote: Counting objects:   8% (14/168)\u001b[K\rremote: Counting objects:   9% (16/168)\u001b[K\rremote: Counting objects:  10% (17/168)\u001b[K\rremote: Counting objects:  11% (19/168)\u001b[K\rremote: Counting objects:  12% (21/168)\u001b[K\rremote: Counting objects:  13% (22/168)\u001b[K\rremote: Counting objects:  14% (24/168)\u001b[K\rremote: Counting objects:  15% (26/168)\u001b[K\rremote: Counting objects:  16% (27/168)\u001b[K\rremote: Counting objects:  17% (29/168)\u001b[K\rremote: Counting objects:  18% (31/168)\u001b[K\rremote: Counting objects:  19% (32/168)\u001b[K\rremote: Counting objects:  20% (34/168)\u001b[K\rremote: Counting objects:  21% (36/168)\u001b[K\rremote: Counting objects:  22% (37/168)\u001b[K\rremote: Counting objects:  23% (39/168)\u001b[K\rremote: Counting objects:  24% (41/168)\u001b[K\rremote: Counting objects:  25% (42/168)\u001b[K\rremote: Counting objects:  26% (44/168)\u001b[K\rremote: Counting objects:  27% (46/168)\u001b[K\rremote: Counting objects:  28% (48/168)\u001b[K\rremote: Counting objects:  29% (49/168)\u001b[K\rremote: Counting objects:  30% (51/168)\u001b[K\rremote: Counting objects:  31% (53/168)\u001b[K\rremote: Counting objects:  32% (54/168)\u001b[K\rremote: Counting objects:  33% (56/168)\u001b[K\rremote: Counting objects:  34% (58/168)\u001b[K\rremote: Counting objects:  35% (59/168)\u001b[K\rremote: Counting objects:  36% (61/168)\u001b[K\rremote: Counting objects:  37% (63/168)\u001b[K\rremote: Counting objects:  38% (64/168)\u001b[K\rremote: Counting objects:  39% (66/168)\u001b[K\rremote: Counting objects:  40% (68/168)\u001b[K\rremote: Counting objects:  41% (69/168)\u001b[K\rremote: Counting objects:  42% (71/168)\u001b[K\rremote: Counting objects:  43% (73/168)\u001b[K\rremote: Counting objects:  44% (74/168)\u001b[K\rremote: Counting objects:  45% (76/168)\u001b[K\rremote: Counting objects:  46% (78/168)\u001b[K\rremote: Counting objects:  47% (79/168)\u001b[K\rremote: Counting objects:  48% (81/168)\u001b[K\rremote: Counting objects:  49% (83/168)\u001b[K\rremote: Counting objects:  50% (84/168)\u001b[K\rremote: Counting objects:  51% (86/168)\u001b[K\rremote: Counting objects:  52% (88/168)\u001b[K\rremote: Counting objects:  53% (90/168)\u001b[K\rremote: Counting objects:  54% (91/168)\u001b[K\rremote: Counting objects:  55% (93/168)\u001b[K\rremote: Counting objects:  56% (95/168)\u001b[K\rremote: Counting objects:  57% (96/168)\u001b[K\rremote: Counting objects:  58% (98/168)\u001b[K\rremote: Counting objects:  59% (100/168)\u001b[K\rremote: Counting objects:  60% (101/168)\u001b[K\rremote: Counting objects:  61% (103/168)\u001b[K\rremote: Counting objects:  62% (105/168)\u001b[K\rremote: Counting objects:  63% (106/168)\u001b[K\rremote: Counting objects:  64% (108/168)\u001b[K\rremote: Counting objects:  65% (110/168)\u001b[K\rremote: Counting objects:  66% (111/168)\u001b[K\rremote: Counting objects:  67% (113/168)\u001b[K\rremote: Counting objects:  68% (115/168)\u001b[K\rremote: Counting objects:  69% (116/168)\u001b[K\rremote: Counting objects:  70% (118/168)\u001b[K\rremote: Counting objects:  71% (120/168)\u001b[K\rremote: Counting objects:  72% (121/168)\u001b[K\rremote: Counting objects:  73% (123/168)\u001b[K\rremote: Counting objects:  74% (125/168)\u001b[K\rremote: Counting objects:  75% (126/168)\u001b[K\rremote: Counting objects:  76% (128/168)\u001b[K\rremote: Counting objects:  77% (130/168)\u001b[K\rremote: Counting objects:  78% (132/168)\u001b[K\rremote: Counting objects:  79% (133/168)\u001b[K\rremote: Counting objects:  80% (135/168)\u001b[K\rremote: Counting objects:  81% (137/168)\u001b[K\rremote: Counting objects:  82% (138/168)\u001b[K\rremote: Counting objects:  83% (140/168)\u001b[K\rremote: Counting objects:  84% (142/168)\u001b[K\rremote: Counting objects:  85% (143/168)\u001b[K\rremote: Counting objects:  86% (145/168)\u001b[K\rremote: Counting objects:  87% (147/168)\u001b[K\rremote: Counting objects:  88% (148/168)\u001b[K\rremote: Counting objects:  89% (150/168)\u001b[K\rremote: Counting objects:  90% (152/168)\u001b[K\rremote: Counting objects:  91% (153/168)\u001b[K\rremote: Counting objects:  92% (155/168)\u001b[K\rremote: Counting objects:  93% (157/168)\u001b[K\rremote: Counting objects:  94% (158/168)\u001b[K\rremote: Counting objects:  95% (160/168)\u001b[K\rremote: Counting objects:  96% (162/168)\u001b[K\rremote: Counting objects:  97% (163/168)\u001b[K\rremote: Counting objects:  98% (165/168)\u001b[K\rremote: Counting objects:  99% (167/168)\u001b[K\rremote: Counting objects: 100% (168/168)\u001b[K\rremote: Counting objects: 100% (168/168), done.\u001b[K\n",
            "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
            "remote: Total 7243 (delta 99), reused 113 (delta 59), pack-reused 7075\u001b[K\n",
            "Receiving objects: 100% (7243/7243), 41.89 MiB | 29.10 MiB/s, done.\n",
            "Resolving deltas: 100% (5049/5049), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygXd5Zo7V0oQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "49a7dbc0-b7dc-4fa7-a022-ef01ba0c7f26"
      },
      "source": [
        "#cd doccano"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/doccano/app/server/doccano\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shpsMp4-V7hQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "626f508d-666d-49ab-e6dc-aabd79c2586d"
      },
      "source": [
        "#! sudo apt-get install libpq-dev"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libpq-dev is already the newest version (10.10-0ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 28 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDhIJOmjV--w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "baf067b9-2fb2-4fa0-c0e7-2a70febe9f1e"
      },
      "source": [
        "#! pip install -r requirements.txt"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: apache-libcloud==2.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (2.4.0)\n",
            "Requirement already satisfied: applicationinsights==0.11.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.11.7)\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: coverage==4.5.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.5.3)\n",
            "Requirement already satisfied: dj-database-url==0.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.5.0)\n",
            "Requirement already satisfied: Django==2.1.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (2.1.7)\n",
            "Requirement already satisfied: django-cloud-browser==0.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (0.5.0)\n",
            "Requirement already satisfied: django-filter==2.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (2.0.0)\n",
            "Requirement already satisfied: django-heroku==0.3.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (0.3.1)\n",
            "Requirement already satisfied: django-webpack-loader==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (0.6.0)\n",
            "Requirement already satisfied: django-widget-tweaks==1.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (1.4.2)\n",
            "Requirement already satisfied: django-polymorphic==2.0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (2.0.3)\n",
            "Collecting django-pyodbc-azure==2.1.0.0 (from -r requirements.txt (line 13))\n",
            "  Using cached https://files.pythonhosted.org/packages/18/ab/133c68bbea94839d8f3b8b4aea4f70e1c6b8ac929aba4adbadc458566a76/django_pyodbc_azure-2.1.0.0-py3-none-any.whl\n",
            "Collecting django-rest-polymorphic==0.1.8 (from -r requirements.txt (line 14))\n",
            "  Using cached https://files.pythonhosted.org/packages/0e/77/b2eef0c31c8c623ef73fcf6cd3c1c7f22d7ee9bbbb0c6ba36ca5a3c9f07a/django_rest_polymorphic-0.1.8-py2.py3-none-any.whl\n",
            "Collecting djangorestframework==3.8.2 (from -r requirements.txt (line 15))\n",
            "  Using cached https://files.pythonhosted.org/packages/90/30/ad1148098ff0c375df2a30cc4494ed953cf7551fc1ecec30fc951c712d20/djangorestframework-3.8.2-py2.py3-none-any.whl\n",
            "Collecting djangorestframework-csv==2.1.0 (from -r requirements.txt (line 16))\n",
            "Collecting djangorestframework-filters==0.10.2 (from -r requirements.txt (line 17))\n",
            "  Using cached https://files.pythonhosted.org/packages/c1/f9/12c26113312212b0869ecabdcf3bfb3c14088a2f4fe2107c3a9975f78a8b/djangorestframework_filters-0.10.2-py2.py3-none-any.whl\n",
            "Collecting environs==4.1.0 (from -r requirements.txt (line 18))\n",
            "  Using cached https://files.pythonhosted.org/packages/1a/cf/266e7875e981a8096500bd8b4858816d482715a7579f3e6501504f036635/environs-4.1.0-py2.py3-none-any.whl\n",
            "Collecting djangorestframework-xml==1.4.0 (from -r requirements.txt (line 19))\n",
            "  Using cached https://files.pythonhosted.org/packages/c3/29/321c2be2995d28d0daf7c91093520b00ed2c7ac1bd5a5b5a744eed76214a/djangorestframework_xml-1.4.0-py2.py3-none-any.whl\n",
            "Collecting Faker==0.9.1 (from -r requirements.txt (line 20))\n",
            "  Using cached https://files.pythonhosted.org/packages/6e/e6/78a4bcf0e59c31b0ed9f4b81ef62cf6b7f4978755b5a88e566dc0fec8e06/Faker-0.9.1-py2.py3-none-any.whl\n",
            "Collecting flake8==3.6.0 (from -r requirements.txt (line 21))\n",
            "  Using cached https://files.pythonhosted.org/packages/34/a6/49e2849a0e5464e1b5d621f63bc8453066f0f367bb3b744a33fca0bc1ddd/flake8-3.6.0-py2.py3-none-any.whl\n",
            "Collecting furl==2.0.0 (from -r requirements.txt (line 22))\n",
            "  Using cached https://files.pythonhosted.org/packages/bd/b6/302ecc007de38274509d6397300afd2e274aba7f1c3c0a165b5f1e1a836a/furl-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: gunicorn==19.9.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 23)) (19.9.0)\n",
            "Collecting lockfile==0.12.2 (from -r requirements.txt (line 24))\n",
            "  Using cached https://files.pythonhosted.org/packages/c8/22/9460e311f340cb62d26a38c419b1381b8593b0bb6b5d1f056938b086d362/lockfile-0.12.2-py2.py3-none-any.whl\n",
            "Collecting mixer==6.1.3 (from -r requirements.txt (line 25))\n",
            "  Using cached https://files.pythonhosted.org/packages/10/c0/5fd76ec8660f0dc4a83fb7e614873bf120b9c3f1c9c35b6185813686eb00/mixer-6.1.3-py2.py3-none-any.whl\n",
            "Collecting model-mommy==1.6.0 (from -r requirements.txt (line 26))\n",
            "  Using cached https://files.pythonhosted.org/packages/0e/e4/19819548d3daadaca6d901eb160cbe63248e18c95f6a05fe03a9ac5d6c93/model_mommy-1.6.0-py2.py3-none-any.whl\n",
            "Collecting psycopg2-binary==2.7.7 (from -r requirements.txt (line 27))\n",
            "  Using cached https://files.pythonhosted.org/packages/2c/85/97db05dd8f6adff57cd1cb8acceeffdaf4724efec18b9af23f3cd75ad089/psycopg2_binary-2.7.7-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting pyexcel==0.5.14 (from -r requirements.txt (line 28))\n",
            "  Using cached https://files.pythonhosted.org/packages/66/38/26f203ede168e4fb6ac9db5a5d212683e553f4f03e1aa2afc93386230f15/pyexcel-0.5.14-py2.py3-none-any.whl\n",
            "Collecting pyexcel-xlsx==0.5.7 (from -r requirements.txt (line 29))\n",
            "  Using cached https://files.pythonhosted.org/packages/95/c0/4cb59318868bb7152688629a28a53f76d02677d310c094ce14a4f7f6f27e/pyexcel_xlsx-0.5.7-py2.py3-none-any.whl\n",
            "Collecting python-dateutil==2.7.3 (from -r requirements.txt (line 30))\n",
            "  Using cached https://files.pythonhosted.org/packages/cf/f5/af2b09c957ace60dcfac112b669c45c8c97e32f94aa8b56da4c6d1682825/python_dateutil-2.7.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytz==2018.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 31)) (2018.4)\n",
            "Requirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 32)) (2.21.0)\n",
            "Collecting six==1.11.0 (from -r requirements.txt (line 33))\n",
            "  Using cached https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl\n",
            "Collecting seqeval==0.0.6 (from -r requirements.txt (line 34))\n",
            "  Using cached https://files.pythonhosted.org/packages/51/79/d2cd36181d1a8843b312766a94b5c83ce451194b1e4cd99fb5dd50fa6760/seqeval-0.0.6-py3-none-any.whl\n",
            "Collecting social-auth-app-django==3.1.0 (from -r requirements.txt (line 35))\n",
            "  Using cached https://files.pythonhosted.org/packages/9f/13/3be586914f69fe9d11beee01b938d329589045dfe90076529c82dae97578/social_auth_app_django-3.1.0-py3-none-any.whl\n",
            "Collecting social-auth-core[azuread]==3.0.0 (from -r requirements.txt (line 36))\n",
            "  Using cached https://files.pythonhosted.org/packages/e0/65/74826e4dc9017ecae356fbdde139075282d835d8a65f8303106a286e13b2/social_auth_core-3.0.0-py3-none-any.whl\n",
            "Collecting text-unidecode==1.2 (from -r requirements.txt (line 37))\n",
            "  Using cached https://files.pythonhosted.org/packages/79/42/d717cc2b4520fb09e45b344b1b0b4e81aa672001dd128c180fabc655c341/text_unidecode-1.2-py2.py3-none-any.whl\n",
            "Collecting tornado==5.0.2 (from -r requirements.txt (line 38))\n",
            "Collecting unittest-xml-reporting==2.5.1 (from -r requirements.txt (line 39))\n",
            "  Using cached https://files.pythonhosted.org/packages/7a/84/43c100dcbb042255ed1105c63a700b0310b48b71b01e939ad5c95f1d512e/unittest_xml_reporting-2.5.1-py2.py3-none-any.whl\n",
            "Collecting vcrpy==2.0.1 (from -r requirements.txt (line 40))\n",
            "  Using cached https://files.pythonhosted.org/packages/10/40/3ae22d519225b2401ba98775b206a798c9e84646599afe0f7cbb4bff10d9/vcrpy-2.0.1-py2.py3-none-any.whl\n",
            "Collecting vcrpy-unittest==0.1.7 (from -r requirements.txt (line 41))\n",
            "  Using cached https://files.pythonhosted.org/packages/26/1f/da7aa0b470f83fd9c7aa1c47f5ebffc63b8ef4018e2da3c0b10a3bcf7521/vcrpy_unittest-0.1.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: whitenoise[brotli]==4.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 42)) (4.1.2)\n",
            "Collecting conllu==1.3.2 (from -r requirements.txt (line 43))\n",
            "  Using cached https://files.pythonhosted.org/packages/56/eb/82b67b01903cc1ff5fae4404bfc53b63ad3ff34ec72a3008591768aee8bb/conllu-1.3.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: psycopg2 in /usr/local/lib/python3.6/dist-packages (from django-heroku==0.3.1->-r requirements.txt (line 9)) (2.7.6.1)\n",
            "Collecting pyodbc>=3.0 (from django-pyodbc-azure==2.1.0.0->-r requirements.txt (line 13))\n",
            "  Using cached https://files.pythonhosted.org/packages/75/29/aa190749bac37ede0f11a68a75e7055254699c11572bd94213f1163dfd8f/pyodbc-4.0.27.tar.gz\n",
            "Collecting unicodecsv (from djangorestframework-csv==2.1.0->-r requirements.txt (line 16))\n",
            "Collecting marshmallow>=2.7.0 (from environs==4.1.0->-r requirements.txt (line 18))\n",
            "  Using cached https://files.pythonhosted.org/packages/78/8c/aa99cd72e69ce14c754a4df752a57faffbd698b14a6fda598a3950273e99/marshmallow-3.2.1-py2.py3-none-any.whl\n",
            "Collecting python-dotenv (from environs==4.1.0->-r requirements.txt (line 18))\n",
            "  Using cached https://files.pythonhosted.org/packages/57/c8/5b14d5cffe7bb06bedf9d66c4562bf90330d3d35e7f0266928c370d9dd6d/python_dotenv-0.10.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: defusedxml>=0.3 in /usr/local/lib/python3.6/dist-packages (from djangorestframework-xml==1.4.0->-r requirements.txt (line 19)) (0.6.0)\n",
            "Collecting pyflakes<2.1.0,>=2.0.0 (from flake8==3.6.0->-r requirements.txt (line 21))\n",
            "  Using cached https://files.pythonhosted.org/packages/44/98/af7a72c9a543b1487d92813c648cb9b9adfbc96faef5455d60f4439aa99b/pyflakes-2.0.0-py2.py3-none-any.whl\n",
            "Collecting pycodestyle<2.5.0,>=2.4.0 (from flake8==3.6.0->-r requirements.txt (line 21))\n",
            "  Using cached https://files.pythonhosted.org/packages/e5/c6/ce130213489969aa58610042dff1d908c25c731c9575af6935c2dfad03aa/pycodestyle-2.4.0-py2.py3-none-any.whl\n",
            "Collecting mccabe<0.7.0,>=0.6.0 (from flake8==3.6.0->-r requirements.txt (line 21))\n",
            "  Using cached https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools>=30 in /usr/local/lib/python3.6/dist-packages (from flake8==3.6.0->-r requirements.txt (line 21)) (41.4.0)\n",
            "Collecting orderedmultidict>=1.0 (from furl==2.0.0->-r requirements.txt (line 22))\n",
            "  Using cached https://files.pythonhosted.org/packages/04/16/5e95c70bda8fe6ea715005c0db8e602400bdba50ae3c72cb380eba551289/orderedmultidict-1.0.1-py2.py3-none-any.whl\n",
            "Collecting texttable>=0.8.2 (from pyexcel==0.5.14->-r requirements.txt (line 28))\n",
            "Collecting lml>=0.0.4 (from pyexcel==0.5.14->-r requirements.txt (line 28))\n",
            "  Using cached https://files.pythonhosted.org/packages/90/e5/eaa5a1c8a9adbac956daa6027789ff13e30362a03cc0d010d88bd867c73f/lml-0.0.9-py2.py3-none-any.whl\n",
            "Collecting pyexcel-io>=0.5.18 (from pyexcel==0.5.14->-r requirements.txt (line 28))\n",
            "  Using cached https://files.pythonhosted.org/packages/50/96/4fd9f1fac8aae6f4f66609c5520b3611bceb631a52a4f6d00c8c4c440270/pyexcel_io-0.5.20-py2.py3-none-any.whl\n",
            "Requirement already satisfied: openpyxl<2.6.0,>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pyexcel-xlsx==0.5.7->-r requirements.txt (line 29)) (2.5.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 32)) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 32)) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 32)) (1.24.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval==0.0.6->-r requirements.txt (line 34)) (1.16.5)\n",
            "Collecting PyJWT>=1.4.0 (from social-auth-core[azuread]==3.0.0->-r requirements.txt (line 36))\n",
            "  Using cached https://files.pythonhosted.org/packages/87/8b/6a9f14b5f781697e51259d81657e6048fd31a113229cf346880bb7545565/PyJWT-1.7.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests-oauthlib>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from social-auth-core[azuread]==3.0.0->-r requirements.txt (line 36)) (1.2.0)\n",
            "Collecting python3-openid>=3.0.10; python_version >= \"3.0\" (from social-auth-core[azuread]==3.0.0->-r requirements.txt (line 36))\n",
            "  Using cached https://files.pythonhosted.org/packages/bd/de/52c5699f52dcee3037db587196dcaf63ffedf5fbeba3183afe9b21a3a89f/python3_openid-3.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: oauthlib>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from social-auth-core[azuread]==3.0.0->-r requirements.txt (line 36)) (3.1.0)\n",
            "Collecting cryptography>=2.1.1; extra == \"azuread\" (from social-auth-core[azuread]==3.0.0->-r requirements.txt (line 36))\n",
            "  Using cached https://files.pythonhosted.org/packages/ca/9a/7cece52c46546e214e10811b36b2da52ce1ea7fa203203a629b8dfadad53/cryptography-2.8-cp34-abi3-manylinux2010_x86_64.whl\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from vcrpy==2.0.1->-r requirements.txt (line 40)) (3.13)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from vcrpy==2.0.1->-r requirements.txt (line 40)) (1.11.2)\n",
            "Collecting yarl; python_version >= \"3.4\" (from vcrpy==2.0.1->-r requirements.txt (line 40))\n",
            "  Using cached https://files.pythonhosted.org/packages/fa/c0/9a73968a9f4e4dac8dffb0ba35f932dd7798fe97901f4942c2d38667862c/yarl-1.3.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: Brotli; extra == \"brotli\" in /usr/local/lib/python3.6/dist-packages (from whitenoise[brotli]==4.1.2->-r requirements.txt (line 42)) (1.0.7)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.6/dist-packages (from openpyxl<2.6.0,>=2.5.0->pyexcel-xlsx==0.5.7->-r requirements.txt (line 29)) (1.4.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.6/dist-packages (from openpyxl<2.6.0,>=2.5.0->pyexcel-xlsx==0.5.7->-r requirements.txt (line 29)) (1.0.1)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.1.1; extra == \"azuread\"->social-auth-core[azuread]==3.0.0->-r requirements.txt (line 36)) (1.13.0)\n",
            "Collecting multidict>=4.0 (from yarl; python_version >= \"3.4\"->vcrpy==2.0.1->-r requirements.txt (line 40))\n",
            "  Using cached https://files.pythonhosted.org/packages/71/cc/ceb5b8c76e7a23212b9e0353053cc35a9d86c763d852a76d9b941fe81fbc/multidict-4.5.2-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.1.1; extra == \"azuread\"->social-auth-core[azuread]==3.0.0->-r requirements.txt (line 36)) (2.19)\n",
            "Building wheels for collected packages: pyodbc\n",
            "  Building wheel for pyodbc (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pyodbc\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for pyodbc\n",
            "Failed to build pyodbc\n",
            "\u001b[31mERROR: python-slugify 3.0.6 has requirement text-unidecode>=1.3, but you'll have text-unidecode 1.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.11.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=4.5.0, but you'll have tornado 5.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement coverage==3.7.1, but you'll have coverage 4.5.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyodbc, django-pyodbc-azure, djangorestframework, six, django-rest-polymorphic, unicodecsv, djangorestframework-csv, djangorestframework-filters, marshmallow, python-dotenv, environs, djangorestframework-xml, text-unidecode, python-dateutil, Faker, pyflakes, pycodestyle, mccabe, flake8, orderedmultidict, furl, lockfile, mixer, model-mommy, psycopg2-binary, texttable, lml, pyexcel-io, pyexcel, pyexcel-xlsx, seqeval, PyJWT, python3-openid, cryptography, social-auth-core, social-auth-app-django, tornado, unittest-xml-reporting, multidict, yarl, vcrpy, vcrpy-unittest, conllu\n",
            "  Running setup.py install for pyodbc ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-6oa27cjk/pyodbc/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-6oa27cjk/pyodbc/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-5fjktz9h/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxFgDHtRWSOO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b7cb3452-4226-4830-8677-51030dc59215"
      },
      "source": [
        "#cd app"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/doccano/app/server/doccano/app\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-yFPs0gWX-e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "83ee1d09-98a8-4135-e1c7-faa1f308a6cf"
      },
      "source": [
        "#cd server/static"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/doccano/app/server/doccano/app/server/static\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEVqu1xCWYDm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "4f810d69-f4ec-4bd4-e948-146a33251cb1"
      },
      "source": [
        "#! npm install"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h\n",
            "> husky@0.13.4 install /content/doccano/app/server/doccano/app/server/static/node_modules/husky\n",
            "> node ./bin/install.js\n",
            "\n",
            "\u001b[36m\u001b[4mhusky\u001b[24m\u001b[39m\n",
            "setting up hooks\n",
            "done\n",
            "\n",
            "\u001b[K\u001b[?25h\n",
            "> swiper@4.3.3 postinstall /content/doccano/app/server/doccano/app/server/static/node_modules/swiper\n",
            "> node -e \"console.log('\\u001b[35m\\u001b[1mLove Swiper? Support Vladimir\\'s work by donating or pledging on patreon:\\u001b[22m\\u001b[39m\\n > \\u001b[32mhttps://patreon.com/vladimirkharlampidi\\u001b[0m\\n')\"\n",
            "\n",
            "\u001b[35m\u001b[1mLove Swiper? Support Vladimir's work by donating or pledging on patreon:\u001b[22m\u001b[39m\n",
            " > \u001b[32mhttps://patreon.com/vladimirkharlampidi\u001b[0m\n",
            "\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m server@1.0.0 No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m server@1.0.0 No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35moptional\u001b[0m SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.9 (node_modules/fsevents):\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.9: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"})\n",
            "\u001b[0m\n",
            "added 966 packages from 766 contributors in 19.489s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Acz6jnBsWfEG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "21ed3906-da59-47f1-8f99-40dbf934be06"
      },
      "source": [
        "#! npm run build"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "> server@1.0.0 build /content/doccano/app/server/doccano/app/server/static\n",
            "> webpack\n",
            "\n",
            "Hash: \u001b[1m85ff092dbdfb9c9f223a\u001b[39m\u001b[22m\n",
            "Version: webpack \u001b[1m4.12.0\u001b[39m\u001b[22m\n",
            "Time: \u001b[1m6745\u001b[39m\u001b[22mms\n",
            "Built at: 2019-10-22 \u001b[1m16:50:41\u001b[39m\u001b[22m\n",
            "                          \u001b[1mAsset\u001b[39m\u001b[22m      \u001b[1mSize\u001b[39m\u001b[22m                        \u001b[1mChunks\u001b[39m\u001b[22m  \u001b[1m\u001b[39m\u001b[22m           \u001b[1m\u001b[39m\u001b[22m\u001b[1mChunk Names\u001b[39m\u001b[22m\n",
            "                       \u001b[1m\u001b[32mindex.js\u001b[39m\u001b[22m   683 KiB                         \u001b[1mindex\u001b[39m\u001b[22m  \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m  index\n",
            "                     \u001b[1m\u001b[32mdataset.js\u001b[39m\u001b[22m   147 KiB                       \u001b[1mdataset\u001b[39m\u001b[22m  \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m  dataset\n",
            "    \u001b[1m\u001b[32mdemo_text_classification.js\u001b[39m\u001b[22m  1.42 MiB      \u001b[1mdemo_text_classification\u001b[39m\u001b[22m  \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m  demo_text_classification\n",
            "            \u001b[1m\u001b[32mdemo_translation.js\u001b[39m\u001b[22m   1.4 MiB              \u001b[1mdemo_translation\u001b[39m\u001b[22m  \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m  demo_translation\n",
            "     \u001b[1m\u001b[32mdocument_classification.js\u001b[39m\u001b[22m  1.33 MiB       \u001b[1mdocument_classification\u001b[39m\u001b[22m  \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m  document_classification\n",
            "            \u001b[1m\u001b[32mdownload_seq2seq.js\u001b[39m\u001b[22m  1.04 MiB              \u001b[1mdownload_seq2seq\u001b[39m\u001b[22m  \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m  download_seq2seq\n",
            "  \u001b[1m\u001b[32mdownload_sequence_labeling.js\u001b[39m\u001b[22m  1.04 MiB    \u001b[1mdownload_sequence_labeling\u001b[39m\u001b[22m  \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m  download_sequence_labeling\n",
            "\u001b[1m\u001b[32mdownload_text_classification.js\u001b[39m\u001b[22m  1.04 MiB  \u001b[1mdownload_text_classification\u001b[39m\u001b[22m  \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m  download_text_classification\n",
            "                   \u001b[1m\u001b[32mguideline.js\u001b[39m\u001b[22m   1.1 MiB                     \u001b[1mguideline\u001b[39m\u001b[22m  \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m  guideline\n",
            "           \u001b[1m\u001b[32mdemo_named_entity.js\u001b[39m\u001b[22m  1.44 MiB             \u001b[1mdemo_named_entity\u001b[39m\u001b[22m  \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m  demo_named_entity\n",
            "                       \u001b[1m\u001b[32mlabel.js\u001b[39m\u001b[22m  1.13 MiB                         \u001b[1mlabel\u001b[39m\u001b[22m  \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m  label\n",
            "                    \u001b[1m\u001b[32mprojects.js\u001b[39m\u001b[22m  1020 KiB                      \u001b[1mprojects\u001b[39m\u001b[22m  \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m  projects\n",
            "                     \u001b[1m\u001b[32mseq2seq.js\u001b[39m\u001b[22m  1.32 MiB                       \u001b[1mseq2seq\u001b[39m\u001b[22m  \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m  seq2seq\n",
            "           \u001b[1m\u001b[32msequence_labeling.js\u001b[39m\u001b[22m  1.36 MiB             \u001b[1msequence_labeling\u001b[39m\u001b[22m  \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m  sequence_labeling\n",
            "                       \u001b[1m\u001b[32mstats.js\u001b[39m\u001b[22m  3.55 MiB                         \u001b[1mstats\u001b[39m\u001b[22m  \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m  stats\n",
            "              \u001b[1m\u001b[32mupload_seq2seq.js\u001b[39m\u001b[22m  1.05 MiB                \u001b[1mupload_seq2seq\u001b[39m\u001b[22m  \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m  upload_seq2seq\n",
            "    \u001b[1m\u001b[32mupload_sequence_labeling.js\u001b[39m\u001b[22m  1.05 MiB      \u001b[1mupload_sequence_labeling\u001b[39m\u001b[22m  \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m  upload_sequence_labeling\n",
            "  \u001b[1m\u001b[32mupload_text_classification.js\u001b[39m\u001b[22m  1.05 MiB    \u001b[1mupload_text_classification\u001b[39m\u001b[22m  \u001b[1m\u001b[32m[emitted]\u001b[39m\u001b[22m  upload_text_classification\n",
            "Entrypoint \u001b[1mdataset\u001b[39m\u001b[22m = \u001b[1m\u001b[32mdataset.js\u001b[39m\u001b[22m\n",
            "Entrypoint \u001b[1mdemo_named_entity\u001b[39m\u001b[22m = \u001b[1m\u001b[32mdemo_named_entity.js\u001b[39m\u001b[22m\n",
            "Entrypoint \u001b[1mdemo_text_classification\u001b[39m\u001b[22m = \u001b[1m\u001b[32mdemo_text_classification.js\u001b[39m\u001b[22m\n",
            "Entrypoint \u001b[1mdemo_translation\u001b[39m\u001b[22m = \u001b[1m\u001b[32mdemo_translation.js\u001b[39m\u001b[22m\n",
            "Entrypoint \u001b[1mdocument_classification\u001b[39m\u001b[22m = \u001b[1m\u001b[32mdocument_classification.js\u001b[39m\u001b[22m\n",
            "Entrypoint \u001b[1mdownload_seq2seq\u001b[39m\u001b[22m = \u001b[1m\u001b[32mdownload_seq2seq.js\u001b[39m\u001b[22m\n",
            "Entrypoint \u001b[1mdownload_sequence_labeling\u001b[39m\u001b[22m = \u001b[1m\u001b[32mdownload_sequence_labeling.js\u001b[39m\u001b[22m\n",
            "Entrypoint \u001b[1mdownload_text_classification\u001b[39m\u001b[22m = \u001b[1m\u001b[32mdownload_text_classification.js\u001b[39m\u001b[22m\n",
            "Entrypoint \u001b[1mguideline\u001b[39m\u001b[22m = \u001b[1m\u001b[32mguideline.js\u001b[39m\u001b[22m\n",
            "Entrypoint \u001b[1mindex\u001b[39m\u001b[22m = \u001b[1m\u001b[32mindex.js\u001b[39m\u001b[22m\n",
            "Entrypoint \u001b[1mlabel\u001b[39m\u001b[22m = \u001b[1m\u001b[32mlabel.js\u001b[39m\u001b[22m\n",
            "Entrypoint \u001b[1mprojects\u001b[39m\u001b[22m = \u001b[1m\u001b[32mprojects.js\u001b[39m\u001b[22m\n",
            "Entrypoint \u001b[1mseq2seq\u001b[39m\u001b[22m = \u001b[1m\u001b[32mseq2seq.js\u001b[39m\u001b[22m\n",
            "Entrypoint \u001b[1msequence_labeling\u001b[39m\u001b[22m = \u001b[1m\u001b[32msequence_labeling.js\u001b[39m\u001b[22m\n",
            "Entrypoint \u001b[1mstats\u001b[39m\u001b[22m = \u001b[1m\u001b[32mstats.js\u001b[39m\u001b[22m\n",
            "Entrypoint \u001b[1mupload_seq2seq\u001b[39m\u001b[22m = \u001b[1m\u001b[32mupload_seq2seq.js\u001b[39m\u001b[22m\n",
            "Entrypoint \u001b[1mupload_sequence_labeling\u001b[39m\u001b[22m = \u001b[1m\u001b[32mupload_sequence_labeling.js\u001b[39m\u001b[22m\n",
            "Entrypoint \u001b[1mupload_text_classification\u001b[39m\u001b[22m = \u001b[1m\u001b[32mupload_text_classification.js\u001b[39m\u001b[22m\n",
            "[\u001b[1m./pages/demo_translation.js\u001b[39m\u001b[22m] 456 bytes {\u001b[1m\u001b[33mdemo_translation\u001b[39m\u001b[22m}\u001b[1m\u001b[32m [built]\u001b[39m\u001b[22m\n",
            "[\u001b[1m./pages/document_classification.js\u001b[39m\u001b[22m] 297 bytes {\u001b[1m\u001b[33mdocument_classification\u001b[39m\u001b[22m}\u001b[1m\u001b[32m [built]\u001b[39m\u001b[22m\n",
            "[\u001b[1m./pages/download_seq2seq.js\u001b[39m\u001b[22m] 195 bytes {\u001b[1m\u001b[33mdownload_seq2seq\u001b[39m\u001b[22m}\u001b[1m\u001b[32m [built]\u001b[39m\u001b[22m\n",
            "[\u001b[1m./pages/download_sequence_labeling.js\u001b[39m\u001b[22m] 232 bytes {\u001b[1m\u001b[33mdownload_sequence_labeling\u001b[39m\u001b[22m}\u001b[1m\u001b[32m [built]\u001b[39m\u001b[22m\n",
            "[\u001b[1m./pages/download_text_classification.js\u001b[39m\u001b[22m] 240 bytes {\u001b[1m\u001b[33mdownload_text_classification\u001b[39m\u001b[22m}\u001b[1m\u001b[32m [built]\u001b[39m\u001b[22m\n",
            "[\u001b[1m./pages/guideline.js\u001b[39m\u001b[22m] 233 bytes {\u001b[1m\u001b[33mguideline\u001b[39m\u001b[22m}\u001b[1m\u001b[32m [built]\u001b[39m\u001b[22m\n",
            "[\u001b[1m./pages/index.js\u001b[39m\u001b[22m] 744 bytes {\u001b[1m\u001b[33mindex\u001b[39m\u001b[22m}\u001b[1m\u001b[32m [built]\u001b[39m\u001b[22m\n",
            "[\u001b[1m./pages/label.js\u001b[39m\u001b[22m] 157 bytes {\u001b[1m\u001b[33mlabel\u001b[39m\u001b[22m}\u001b[1m\u001b[32m [built]\u001b[39m\u001b[22m\n",
            "[\u001b[1m./pages/projects.js\u001b[39m\u001b[22m] 171 bytes {\u001b[1m\u001b[33mprojects\u001b[39m\u001b[22m}\u001b[1m\u001b[32m [built]\u001b[39m\u001b[22m\n",
            "[\u001b[1m./pages/seq2seq.js\u001b[39m\u001b[22m] 197 bytes {\u001b[1m\u001b[33mseq2seq\u001b[39m\u001b[22m}\u001b[1m\u001b[32m [built]\u001b[39m\u001b[22m\n",
            "[\u001b[1m./pages/sequence_labeling.js\u001b[39m\u001b[22m] 273 bytes {\u001b[1m\u001b[33msequence_labeling\u001b[39m\u001b[22m}\u001b[1m\u001b[32m [built]\u001b[39m\u001b[22m\n",
            "[\u001b[1m./pages/stats.js\u001b[39m\u001b[22m] 154 bytes {\u001b[1m\u001b[33mstats\u001b[39m\u001b[22m}\u001b[1m\u001b[32m [built]\u001b[39m\u001b[22m\n",
            "[\u001b[1m./pages/upload_seq2seq.js\u001b[39m\u001b[22m] 187 bytes {\u001b[1m\u001b[33mupload_seq2seq\u001b[39m\u001b[22m}\u001b[1m\u001b[32m [built]\u001b[39m\u001b[22m\n",
            "[\u001b[1m./pages/upload_sequence_labeling.js\u001b[39m\u001b[22m] 224 bytes {\u001b[1m\u001b[33mupload_sequence_labeling\u001b[39m\u001b[22m}\u001b[1m\u001b[32m [built]\u001b[39m\u001b[22m\n",
            "[\u001b[1m./pages/upload_text_classification.js\u001b[39m\u001b[22m] 232 bytes {\u001b[1m\u001b[33mupload_text_classification\u001b[39m\u001b[22m}\u001b[1m\u001b[32m [built]\u001b[39m\u001b[22m\n",
            "    + 347 hidden modules\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoxS13j4Wl0d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cc0e738c-7823-47d5-c2f5-2b9f57260d55"
      },
      "source": [
        "#cd .."
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/doccano/app/server/doccano/app/server\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0VQFyyzY6U6",
        "colab_type": "text"
      },
      "source": [
        "# 3)- Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H56kdqqIUhRy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ad591ba8-14c7-47d6-b4c3-ab5c5fe10497"
      },
      "source": [
        "# Read export file\n",
        "with open('doccano_export.json', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "print(json.loads(lines[1]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'id': 35, 'text': \"Now, I'm ready to move this to a serving environment (via Sagemaker, but that just implements tensorflow.serving).\", 'annotations': [{'label': 4, 'start_offset': 0, 'end_offset': 3, 'user': 1}, {'label': 4, 'start_offset': 5, 'end_offset': 8, 'user': 1}, {'label': 4, 'start_offset': 9, 'end_offset': 14, 'user': 1}, {'label': 4, 'start_offset': 15, 'end_offset': 17, 'user': 1}, {'label': 4, 'start_offset': 18, 'end_offset': 22, 'user': 1}, {'label': 4, 'start_offset': 23, 'end_offset': 27, 'user': 1}, {'label': 4, 'start_offset': 28, 'end_offset': 30, 'user': 1}, {'label': 4, 'start_offset': 31, 'end_offset': 32, 'user': 1}, {'label': 4, 'start_offset': 33, 'end_offset': 40, 'user': 1}, {'label': 4, 'start_offset': 41, 'end_offset': 52, 'user': 1}, {'label': 4, 'start_offset': 54, 'end_offset': 57, 'user': 1}, {'label': 4, 'start_offset': 58, 'end_offset': 67, 'user': 1}, {'label': 4, 'start_offset': 69, 'end_offset': 72, 'user': 1}, {'label': 4, 'start_offset': 73, 'end_offset': 77, 'user': 1}, {'label': 4, 'start_offset': 78, 'end_offset': 82, 'user': 1}, {'label': 4, 'start_offset': 83, 'end_offset': 93, 'user': 1}, {'label': 2, 'start_offset': 94, 'end_offset': 112, 'user': 1}], 'meta': {}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA7rj01BYzSj",
        "colab_type": "text"
      },
      "source": [
        "### 3.1)-Split out words and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0UuoQiYVZo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The numerical doccano label to actual label (B-I-O scheme)\n",
        "ix_to_label = {4: 'O', 3: 'I', 2: 'B'}\n",
        "    \n",
        "# train/test data\n",
        "data = []\n",
        "\n",
        "# Vocabulary\n",
        "vocab = set()\n",
        "    \n",
        "# Loop over each data point (a corpus of labeled text) to extract words\n",
        "for line in lines:\n",
        "    # An ordered dict will keep items in order for further manipulation\n",
        "    # so we initialize here\n",
        "    orddict = OrderedDict({})\n",
        "    # Lists to hold the words and labels\n",
        "    words = []\n",
        "    labels = []\n",
        "    # Convert line to json\n",
        "    injson = json.loads(line)\n",
        "    annots = injson['annotations']\n",
        "    text = injson['text']\n",
        "    \n",
        "    # Add each word annotation to OrderedDict\n",
        "    for ann in annots:\n",
        "        orddict[ann['start_offset']] = ann\n",
        "    \n",
        "    # Sort ordered dict because there's no guarantee reading json\n",
        "    # maintained order\n",
        "    orddict = sorted(orddict.items(), key=lambda x: x[1]['start_offset'])\n",
        "    \n",
        "    for item in orddict:\n",
        "        # the item is a tuple where second value is the actual value we want\n",
        "        ann = item[1]\n",
        "        # Subset text string\n",
        "        word = text[ann['start_offset']:(ann['end_offset'] + 1)].rstrip()\n",
        "        label = ix_to_label[ann['label']]\n",
        "        # Add to list for this datum/corpus\n",
        "        words.append(word)\n",
        "        labels.append(label)\n",
        "        vocab.add(word)\n",
        "    # Add to overall data containers\n",
        "    data.append((words, labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r31WlyHuZEXq",
        "colab_type": "text"
      },
      "source": [
        "### 3.2)-Split into train and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC_xL1SuY4Xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_train = math.floor(len(data) * 0.8) # 80% to train\n",
        "training_data, test_data = data[:num_train], data[num_train:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFLMHpLoZKOp",
        "colab_type": "text"
      },
      "source": [
        "# 4)- Building model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap2tQ-fPZOCo",
        "colab_type": "text"
      },
      "source": [
        "### 4.1)- Tags and Hyper-parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctDGXC2pZH2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "EMBEDDING_DIM = 5\n",
        "HIDDEN_DIM = 4\n",
        "MINIBATCH_SIZE = 2\n",
        "LEARNING_WEIGHT = 5e-2\n",
        "WEIGHT_DECAY = 1e-4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40aS3wkvdDSj",
        "colab_type": "text"
      },
      "source": [
        "### 4.2)- CRF Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHacGFJ_ZTZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        \"\"\"Initialize network.\"\"\"\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0TIDoDpdH0p",
        "colab_type": "text"
      },
      "source": [
        "### 4.3)- Function to initialize hidden state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaWizjxpZV6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%add_to BiLSTM_CRF\n",
        "\n",
        "def init_hidden(self):\n",
        "    \"\"\"Two tensors to hold hidden states, one for each\n",
        "    LSTM direction with dimensions of (num_layers, \n",
        "    minibatch, hidden_dim)\"\"\"\n",
        "    # Minibatch small because small dataset below\n",
        "    return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
        "            torch.randn(2, 1, self.hidden_dim // 2).to(device))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS3gop_ddP44",
        "colab_type": "text"
      },
      "source": [
        "### 4.4)- Conditional Random Field\n",
        "\n",
        "In this case, iteration has been made per sentence i.e one sentence is one training data point.For each sentence, there is multiple embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srzUHRJDZYHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%add_to BiLSTM_CRF\n",
        "\n",
        "\n",
        "def _forward_alg(self, feats):\n",
        "    \"\"\"Core magic of the Conditional Random Field.  \n",
        "    \n",
        "    Input:\n",
        "        The word embeddeding vectors for a sentence\n",
        "    \n",
        "    Since were using PyTorch to compute gradients for us, \n",
        "    we technically only need the forward part of the forward-backward \n",
        "    algorithm \"\"\"\n",
        "    # Do the forward algorithm to compute the partition function\n",
        "    init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "    # START_TAG (\"<START>\") has all of the score.\n",
        "    init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "    forward_var = init_alphas\n",
        "\n",
        "    # Iterate through the sentence\n",
        "    for feat in feats:\n",
        "        alphas_t = []  # The forward tensors at this timestep\n",
        "        for next_tag in range(self.tagset_size):\n",
        "            # broadcast the emission score: it is the same regardless of\n",
        "            # the previous tag\n",
        "            emit_score = feat[next_tag].view(\n",
        "                1, -1).expand(1, self.tagset_size)\n",
        "            # the ith entry of trans_score is the score of transitioning to\n",
        "            # next_tag from i\n",
        "            trans_score = self.transitions[next_tag].view(1, -1)\n",
        "            # The ith entry of next_tag_var is the value for the\n",
        "            # edge (i -> next_tag) before we do log-sum-exp\n",
        "            next_tag_var = forward_var + trans_score + emit_score\n",
        "            # The forward variable for this tag is log-sum-exp of all the\n",
        "            # scores.\n",
        "            alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "        forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "    terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "    alpha = log_sum_exp(terminal_var)\n",
        "    return alpha"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbr6NXUodc_Q",
        "colab_type": "text"
      },
      "source": [
        "### 4.5)- For LSTM features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXlJ3p7bZanG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%add_to BiLSTM_CRF\n",
        "\n",
        "def _get_lstm_features(self, sentence):\n",
        "    \"\"\"Compute output vector of BiLSTM - used in \n",
        "    the forward pass of network\"\"\"\n",
        "    self.hidden = self.init_hidden()\n",
        "    embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "    lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "    lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "    # Map LSTM features into tag space\n",
        "    lstm_feats = self.hidden2tag(lstm_out)\n",
        "    return lstm_feats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJvxoy1Adju2",
        "colab_type": "text"
      },
      "source": [
        "### 4.6)- Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU4-RSwcZc1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%add_to BiLSTM_CRF\n",
        "\n",
        "def _score_sentence(self, feats, tags):\n",
        "    \"\"\"Gives the score of a provided tag sequence\"\"\"\n",
        "    # Gives the score of a provided tag sequence\n",
        "    score = torch.zeros(1).to(device)\n",
        "    tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), \n",
        "                      tags])\n",
        "    for i, feat in enumerate(feats):\n",
        "        score = score + \\\n",
        "            self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "    score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "    return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4UAP2a_dl-e",
        "colab_type": "text"
      },
      "source": [
        "### 4.7)- Viterbi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfTWEnuKZfEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%add_to BiLSTM_CRF\n",
        "\n",
        "def _viterbi_decode(self, feats):\n",
        "    \"\"\"Implements Viterbi algorithm for finding most likely sequence of labels.\n",
        "    Used in the forward pass of the network.\n",
        "\n",
        "    We take the maximum over the previous states as opposed to the sum. \n",
        "    Input:\n",
        "        loglikelihoods: torch tensor.\n",
        "    Output:\n",
        "        tuple. The first entry is the loglikelihood of this sequence. The second is \n",
        "        the most likely sequence of labels. \n",
        "    \"\"\"\n",
        "    backpointers = []\n",
        "\n",
        "    # Initialize the viterbi variables in log space\n",
        "    init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "    init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "    # forward_var at step i holds the viterbi variables for step i-1\n",
        "    forward_var = init_vvars\n",
        "    for feat in feats:\n",
        "        bptrs_t = []  # holds the backpointers for this step\n",
        "        viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "        for next_tag in range(self.tagset_size):\n",
        "            # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "            # previous step, plus the score of transitioning\n",
        "            # from tag i to next_tag.\n",
        "            # We don't include the emission scores here because the max\n",
        "            # does not depend on them (we add them in below)\n",
        "            next_tag_var = forward_var + self.transitions[next_tag]\n",
        "            best_tag_id = argmax(next_tag_var)\n",
        "            bptrs_t.append(best_tag_id)\n",
        "            viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "        # Now add in the emission scores, and assign forward_var to the set\n",
        "        # of viterbi variables we just computed\n",
        "        forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1).to(device)\n",
        "        backpointers.append(bptrs_t)\n",
        "\n",
        "    # Transition to STOP_TAG\n",
        "    terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "    best_tag_id = argmax(terminal_var)\n",
        "    path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "    # Follow the back pointers to decode the best path.\n",
        "    best_path = [best_tag_id]\n",
        "    for bptrs_t in reversed(backpointers):\n",
        "        best_tag_id = bptrs_t[best_tag_id]\n",
        "        best_path.append(best_tag_id)\n",
        "    # Pop off the start tag (we dont want to return that to the caller)\n",
        "    start = best_path.pop()\n",
        "    assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "    best_path.reverse()\n",
        "    return path_score, best_path\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWqRhTp7dsh-",
        "colab_type": "text"
      },
      "source": [
        "### 4.8)- Negative Log Likelihood"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yjo6CatZi_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%add_to BiLSTM_CRF\n",
        "\n",
        "def neg_log_likelihood(self, sentence, tags):\n",
        "    \"\"\"Calculate the negative log likelihood given a sequence and labels.\n",
        "    This is used in training (only) because we don't need to create\n",
        "    and check the B-I-O tags themselves - only the score is important\n",
        "    here for calculating the loss.\"\"\"\n",
        "    feats = self._get_lstm_features(sentence)\n",
        "    forward_score = self._forward_alg(feats)\n",
        "    gold_score = self._score_sentence(feats, tags)\n",
        "    return forward_score - gold_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6vYzWg4dyAd",
        "colab_type": "text"
      },
      "source": [
        "### 4.9)- Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmJRECKCZlev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%add_to BiLSTM_CRF\n",
        "\n",
        "def forward(self, sentence):\n",
        "    \"\"\"The forward pass function for training the network.\n",
        "    This is used in inference only.\"\"\"\n",
        "    # Get the emission scores (output layer) from the \n",
        "    # BiLSTM \n",
        "    lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "    # Find the best path, given the features.\n",
        "    score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "    return score, tag_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB85tJeVf0Gi",
        "colab_type": "text"
      },
      "source": [
        "**End of our class. %%add is adding whole story from one cell to other**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-4P3_eDd1Tt",
        "colab_type": "text"
      },
      "source": [
        "### 4.10)- Removing punctuation from text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zSQfvaIZnd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_punct(text):\n",
        "    \"\"\"Remove punctuation from a piece of text\"\"\"\n",
        "    punct = list(\".,()-\")\n",
        "    for p in punct:\n",
        "        text = text.replace(p, '')\n",
        "    return text\n",
        "    \n",
        "text = remove_punct(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jThl0V9wd8lF",
        "colab_type": "text"
      },
      "source": [
        "### 4.11)-lookup dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dax6A8n0Z9q_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a lookup dict for all possible words and record their index\n",
        "word_to_ix = {k: v for (k, v) in zip(vocab, range(len(vocab)))}\n",
        "tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
        "ix_to_tag = {0: \"B\", 1: \"I\", 2: \"O\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze8HZjt_eDfW",
        "colab_type": "text"
      },
      "source": [
        "### 4.12)-Initialize model and optimizer for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WunpK0y8aABg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
        "model = model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_WEIGHT, weight_decay=WEIGHT_DECAY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_gyao87aDLX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "8499a255-303e-4a0e-c7a3-35a626f5281e"
      },
      "source": [
        "# Check predictions before training\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    precheck_sent = precheck_sent.to(device)\n",
        "    pred =  model(precheck_sent)[1]\n",
        "    print('Prediction:   ', [ix_to_tag[idx] for idx in pred])\n",
        "    print('Ground truth: ', training_data[0][1])\n",
        "    print(training_data[0][0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction:    ['O', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'O', 'B', 'I']\n",
            "Ground truth:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B']\n",
            "['This', 'makes', 'it', 'harder', 'to', 'understand', 'the', 'behavior', 'of', 'the', 'function', 'tf.scatter_add', 'in', 'case', 'indices', 'is', 'a', 'matrix.', 'Specifically,', 'what', 'is', 'the', 'difference', 'between', 'tf.scatter_add', 'and', 'tf.scatter_nd w', 'when', 'indices', 'is', 'a', 'matrix.', 'This', 'will', 'raise', 'an', 'error', 'that', 'only', 'sequential', 'or', 'functional', 'models', 'can', 'be', 'saved', \"model.save('custom_model.h5')\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkVPQm3Eg4yj",
        "colab_type": "text"
      },
      "source": [
        "Not a very good job at this point. Hopefully, we will get better results after training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NgzE-DxaIL6",
        "colab_type": "text"
      },
      "source": [
        "# 5)-Train model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDmYL51paJmv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "ad3cd480-4143-40f7-a834-57fe29746e08"
      },
      "source": [
        "%%time\n",
        "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
        "# again, normally you would do more than 300 epochs, but we have\n",
        "# toy data\n",
        "\n",
        "losses = []\n",
        "for epoch in range(100):  \n",
        "    for sentence, tags in training_data:\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance of LSTM\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
        "        sentence_in, targets = sentence_in.to(device), targets.to(device)\n",
        "\n",
        "        # Step 3. A lot happens.  Run our forward pass to get features from BLSTM,\n",
        "        # run the CRF and get the negative log likelihoods and find the best \n",
        "        # \"path\" through sentence with the tags using the viterbi algorithm \n",
        "        # (also part of forward pass).\n",
        "        # BTW our dynamic computational graph is created with the forward pass\n",
        "        # Returns the forward score - ground truth score (our loss measure)\n",
        "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "\n",
        "        # Step 4. Compute the loss, gradients (backprop), and update the \n",
        "        # parameters by calling optimizer.step() - optimizer here is \n",
        "        # SGD for our CRF\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(\"Epoch: {} Loss: {}\".format(epoch+1, np.mean(losses)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10 Loss: 9.635039234161377\n",
            "Epoch: 20 Loss: 8.520493650436402\n",
            "Epoch: 30 Loss: 7.006024519602458\n",
            "Epoch: 40 Loss: 5.711645150184632\n",
            "Epoch: 50 Loss: 4.711056823730469\n",
            "Epoch: 60 Loss: 3.9849269866943358\n",
            "Epoch: 70 Loss: 3.4500631604875838\n",
            "Epoch: 80 Loss: 3.033038914203644\n",
            "Epoch: 90 Loss: 2.7100445641411675\n",
            "Epoch: 100 Loss: 2.4464377975463867\n",
            "CPU times: user 1min 34s, sys: 1.21 s, total: 1min 35s\n",
            "Wall time: 1min 36s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odRfVHhzkp_x",
        "colab_type": "text"
      },
      "source": [
        "### Save model\n",
        "\n",
        "\n",
        "save weights not shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8uQQBhuaQHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), 'model_0.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYefffdMhh70",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "3b8958fe-9c14-434e-918e-281c0e59f18a"
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTM_CRF(\n",
              "  (word_embeds): Embedding(182, 5)\n",
              "  (lstm): LSTM(5, 2, bidirectional=True)\n",
              "  (hidden2tag): Linear(in_features=4, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZEFUrR2aQLK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "73ba9414-f516-4e9d-eb3d-28122829eb77"
      },
      "source": [
        "# Sanity check for predictions after training\n",
        "# No need to accumulate gradients because this is a validation\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[1][0], word_to_ix)\n",
        "    precheck_sent = precheck_sent.to(device)\n",
        "    pred =  model(precheck_sent)[1]\n",
        "    print('Prediction:   ', [ix_to_tag[idx] for idx in pred])\n",
        "    print('Ground truth: ', training_data[1][1])\n",
        "    print(training_data[1][0])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction:    ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B']\n",
            "Ground truth:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B']\n",
            "['Now,', \"I'm\", 'ready', 'to', 'move', 'this', 'to', 'a', 'serving', 'environment', 'via', 'Sagemaker,', 'but', 'that', 'just', 'implements', 'tensorflow.serving)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucwpGBgbk7KO",
        "colab_type": "text"
      },
      "source": [
        "with *torch.no_grad()* is used so that gradients are not propagated which is important for when we are running predictions and scoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdYWN2hXaW-6",
        "colab_type": "text"
      },
      "source": [
        "# 6)-Evaluate\n",
        "\n",
        "Let's test our model on an unseen sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsJLaeVTaQNx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "b6c12dc6-37d4-4349-c1db-049da6b57685"
      },
      "source": [
        "model.eval() # set again just in case\n",
        "\n",
        "# Pick some test data\n",
        "test_datum = test_data[0][0]\n",
        "test_text = test_data[0][1]\n",
        "\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(test_datum, word_to_ix)\n",
        "    precheck_sent = precheck_sent.to(device)\n",
        "    pred =  model(precheck_sent)[1]\n",
        "    print('Prediction:   ', [ix_to_tag[idx] for idx in pred])\n",
        "    print('Ground truth: ', test_text)\n",
        "    print('Text: ', test_datum)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction:    ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Ground truth:  ['O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text:  ['I', 'have', 'a', 'tf.data.Dataset', 'that', 'I', 'want', 'to', 'write', 'to', 'tfrecord', 'files.', 'to', 'do', 'this,', 'I', 'currently', 'use', 'tf.python_io.TFRecordWriter', 'to', 'do', 'this,', 'but', 'would', 'like', 'to', 'use', 'tf.data.experimental.TFRecordWriter,', 'as', 'it', 'would', 'be', 'more', 'efficient', 'to', 'also', 'do', 'the', 'writing', 'as', 'part', 'of', 'the', 'dataset', 'graph', 'execution.']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}