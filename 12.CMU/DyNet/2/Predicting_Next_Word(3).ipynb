{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "Predicting_Next_Word(3).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFBOqODHw6mV",
        "colab_type": "text"
      },
      "source": [
        "# Using Linear model\n",
        "\n",
        "- Applying efficiency trick concepts\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70Vdw8_niYm0",
        "colab_type": "text"
      },
      "source": [
        "# 1)-Importing key Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9CuXeO1xlV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#support both Python 2 and Python 3 with minimal overhead.\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# I am an engineer. I care only about error not warning. So, let's be maverick and ignore warnings.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8TutW-ygWhX",
        "colab_type": "code",
        "outputId": "7d741675-6154-41f7-a5b3-a4d9bb6e717a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "! pip install dynet"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dynet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/f0/01a561a301a8ea9aea1c28f82e108c38cd103964c7a46286ab01757a4092/dyNET-2.1-cp36-cp36m-manylinux1_x86_64.whl (28.1MB)\n",
            "\u001b[K     |████████████████████████████████| 28.1MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from dynet) (1.17.4)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from dynet) (0.29.14)\n",
            "Installing collected packages: dynet\n",
            "Successfully installed dynet-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiA4L15Iw7kb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import dynet as dy\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcCkkT0mx2Yc",
        "colab_type": "code",
        "outputId": "a8047b62-66d6-4070-ad15-6636ea135b34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "! pip install version_information"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting version_information\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b0/6088e15b9ac43a08ccd300d68e0b900a20cf62077596c11ad11dd8cc9e4b/version_information-1.0.3.tar.gz\n",
            "Building wheels for collected packages: version-information\n",
            "  Building wheel for version-information (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for version-information: filename=version_information-1.0.3-cp36-none-any.whl size=3880 sha256=d561c584e1b8e06c9700b1153ae4c3b7d2647c612fbb10bb10b00f9e805497da\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/4c/b3/1976ac11dbd802723b564de1acaa453a72c36c95827e576321\n",
            "Successfully built version-information\n",
            "Installing collected packages: version-information\n",
            "Successfully installed version-information-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z56btwC7x268",
        "colab_type": "code",
        "outputId": "4cff914e-76ad-475c-83a0-0f6e05dc2383",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "# first install: pip install version_information\n",
        "%reload_ext version_information\n",
        "%version_information pandas,torch,numpy"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "\\begin{tabular}{|l|l|}\\hline\n{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\nPython & 3.6.8 64bit [GCC 8.3.0] \\\\ \\hline\nIPython & 5.5.0 \\\\ \\hline\nOS & Linux 4.14.137+ x86\\_64 with Ubuntu 18.04 bionic \\\\ \\hline\npandas & 0.25.3 \\\\ \\hline\ntorch & 1.3.1+cu100 \\\\ \\hline\nnumpy & 1.17.4 \\\\ \\hline\n\\hline \\multicolumn{2}{|l|}{Sun Nov 17 20:01:57 2019 UTC} \\\\ \\hline\n\\end{tabular}\n",
            "application/json": {
              "Software versions": [
                {
                  "version": "3.6.8 64bit [GCC 8.3.0]",
                  "module": "Python"
                },
                {
                  "version": "5.5.0",
                  "module": "IPython"
                },
                {
                  "version": "Linux 4.14.137+ x86_64 with Ubuntu 18.04 bionic",
                  "module": "OS"
                },
                {
                  "version": "0.25.3",
                  "module": "pandas"
                },
                {
                  "version": "1.3.1+cu100",
                  "module": "torch"
                },
                {
                  "version": "1.17.4",
                  "module": "numpy"
                }
              ]
            },
            "text/html": [
              "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.6.8 64bit [GCC 8.3.0]</td></tr><tr><td>IPython</td><td>5.5.0</td></tr><tr><td>OS</td><td>Linux 4.14.137+ x86_64 with Ubuntu 18.04 bionic</td></tr><tr><td>pandas</td><td>0.25.3</td></tr><tr><td>torch</td><td>1.3.1+cu100</td></tr><tr><td>numpy</td><td>1.17.4</td></tr><tr><td colspan='2'>Sun Nov 17 20:01:57 2019 UTC</td></tr></table>"
            ],
            "text/plain": [
              "Software versions\n",
              "Python 3.6.8 64bit [GCC 8.3.0]\n",
              "IPython 5.5.0\n",
              "OS Linux 4.14.137+ x86_64 with Ubuntu 18.04 bionic\n",
              "pandas 0.25.3\n",
              "torch 1.3.1+cu100\n",
              "numpy 1.17.4\n",
              "Sun Nov 17 20:01:57 2019 UTC"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw-J6S79LU8O",
        "colab_type": "text"
      },
      "source": [
        "# 2)- Setting up neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFX88YgmLWsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = 2 # The length of the n-gram\n",
        "EMB_SIZE = 128 # The size of the embedding\n",
        "HID_SIZE = 128 # The size of the hidden layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGXhZjWd7PKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions to read in the corpus\n",
        "w2i = defaultdict(lambda: len(w2i))\n",
        "S = w2i[\"<s>\"]\n",
        "UNK = w2i[\"<unk>\"]\n",
        "def read_dataset(filename):\n",
        "  with open(filename, \"r\") as f:\n",
        "    for line in f:\n",
        "      yield [w2i[x] for x in line.strip().split(\" \")]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTa40YTaxRXk",
        "colab_type": "text"
      },
      "source": [
        "# 3)- Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwirF9WxNbAY",
        "colab_type": "text"
      },
      "source": [
        "### loading data using traditional format\n",
        "using read()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XguR09WKw5uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = list(read_dataset(\"train.txt\"))\n",
        "w2i = defaultdict(lambda: UNK, w2i)\n",
        "dev = list(read_dataset(\"valid.txt\"))\n",
        "i2w = {v: k for k, v in w2i.items()}\n",
        "nwords = len(w2i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL8Mi4OWMWtT",
        "colab_type": "text"
      },
      "source": [
        "# 4)- Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPuE-EyrMZeT",
        "colab_type": "text"
      },
      "source": [
        "### 4.1)- Start DyNet and define trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT3XMPH3Mex7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = dy.ParameterCollection()\n",
        "trainer = dy.AdamTrainer(model, alpha=0.001) # notice how LR is changed alpha as we are using ADAM now"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_JwS6zcMmZT",
        "colab_type": "text"
      },
      "source": [
        "### 4.2)-Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKI5ySThMe0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W_emb = model.add_lookup_parameters((nwords, EMB_SIZE)) # Word weights at each position\n",
        "W_h = model.add_parameters((HID_SIZE, EMB_SIZE * N))    # Weights of the softmax\n",
        "b_h = model.add_parameters((HID_SIZE))                  # Weights of the softmax\n",
        "W_sm = model.add_parameters((nwords, HID_SIZE))         # Weights of the softmax\n",
        "b_sm = model.add_parameters((nwords))                   # Softmax bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxGBAs7aMuhz",
        "colab_type": "text"
      },
      "source": [
        "### 4.3)-A function to calculate scores for one value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC_uuRKvMe32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_score_of_histories(words, dropout=0.0):\n",
        "  # This will change from a list of histories, to a list of words in each history position\n",
        "  words = np.transpose(words)\n",
        "  # Lookup the embeddings and concatenate them\n",
        "  emb = dy.concatenate([dy.lookup_batch(W_emb, x) for x in words])\n",
        "  # Create the hidden layer\n",
        "  h = dy.tanh(dy.affine_transform([b_h, W_h, emb]))\n",
        "  # Perform dropout\n",
        "  if dropout != 0.0:\n",
        "    h = dy.dropout(h, dropout)\n",
        "  # Calculate the score and return\n",
        "  return dy.affine_transform([b_sm, W_sm, h])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Oo9CdoWM3DM",
        "colab_type": "text"
      },
      "source": [
        "### 4.4)-Calculate the loss value for the entire sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg0yiErZMe6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_sent_loss(sent, dropout=0.0):\n",
        "  # Create a computation graph\n",
        "  dy.renew_cg()\n",
        "  # The initial history is equal to end of sentence symbols\n",
        "  hist = [S] * N\n",
        "  # Step through the sentence, including the end of sentence token\n",
        "  all_histories = []\n",
        "  all_targets = []\n",
        "  for next_word in sent + [S]:\n",
        "    all_histories.append(list(hist))\n",
        "    all_targets.append(next_word)\n",
        "    hist = hist[1:] + [next_word]\n",
        "  s = calc_score_of_histories(all_histories, dropout=dropout)\n",
        "  return dy.sum_batches(dy.pickneglogsoftmax_batch(s, all_targets))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8b1JUI0hvhk",
        "colab_type": "text"
      },
      "source": [
        "### 4.5)-Generate a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-xgWIVrh0pc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-8xXLvEMe87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_sent():\n",
        "  dy.renew_cg()\n",
        "  hist = [S] * N\n",
        "  sent = []\n",
        "  while True:\n",
        "    p = dy.softmax(calc_score_of_histories([hist])).npvalue()\n",
        "    next_word = np.random.choice(nwords, p=p/p.sum())\n",
        "    if next_word == S or len(sent) == MAX_LEN:\n",
        "      break\n",
        "    sent.append(next_word)\n",
        "    hist = hist[1:] + [next_word]\n",
        "  return sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Huvgc_Gh959",
        "colab_type": "text"
      },
      "source": [
        "# 5)- Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbAdt9WD-LYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last_dev = 1e20\n",
        "best_dev = 1e20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6BKC7H9iAxM",
        "colab_type": "code",
        "outputId": "3c9a8286-44e3-434e-af81-3034aba88563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        }
      },
      "source": [
        "for ITER in range(5):\n",
        "  # Perform training\n",
        "  random.shuffle(train)\n",
        "  train_words, train_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  for sent_id, sent in enumerate(train):\n",
        "    my_loss = calc_sent_loss(sent, dropout=0.2)\n",
        "    train_loss += my_loss.value()\n",
        "    train_words += len(sent)\n",
        "    my_loss.backward()\n",
        "    trainer.update()\n",
        "    if (sent_id+1) % 5000 == 0:\n",
        "      print(\"--finished %r sentences (word/sec=%.2f)\" % (sent_id+1, train_words/(time.time()-start)))\n",
        "  print(\"iter %r: train loss/word=%.4f, ppl=%.4f (word/sec=%.2f)\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words), train_words/(time.time()-start)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--finished 5000 sentences (word/sec=1292.70)\n",
            "--finished 10000 sentences (word/sec=1289.24)\n",
            "--finished 15000 sentences (word/sec=1291.28)\n",
            "--finished 20000 sentences (word/sec=1294.41)\n",
            "--finished 25000 sentences (word/sec=1295.21)\n",
            "--finished 30000 sentences (word/sec=1292.46)\n",
            "--finished 35000 sentences (word/sec=1290.20)\n",
            "--finished 40000 sentences (word/sec=1290.51)\n",
            "iter 0: train loss/word=6.0528, ppl=425.2811 (word/sec=1290.74)\n",
            "--finished 5000 sentences (word/sec=1292.57)\n",
            "--finished 10000 sentences (word/sec=1293.89)\n",
            "--finished 15000 sentences (word/sec=1295.00)\n",
            "--finished 20000 sentences (word/sec=1294.78)\n",
            "--finished 25000 sentences (word/sec=1294.68)\n",
            "--finished 30000 sentences (word/sec=1295.42)\n",
            "--finished 35000 sentences (word/sec=1296.39)\n",
            "--finished 40000 sentences (word/sec=1298.12)\n",
            "iter 1: train loss/word=5.6172, ppl=275.1247 (word/sec=1297.84)\n",
            "--finished 5000 sentences (word/sec=1288.87)\n",
            "--finished 10000 sentences (word/sec=1298.70)\n",
            "--finished 15000 sentences (word/sec=1300.99)\n",
            "--finished 20000 sentences (word/sec=1302.22)\n",
            "--finished 25000 sentences (word/sec=1302.35)\n",
            "--finished 30000 sentences (word/sec=1305.38)\n",
            "--finished 35000 sentences (word/sec=1307.00)\n",
            "--finished 40000 sentences (word/sec=1307.50)\n",
            "iter 2: train loss/word=5.4594, ppl=234.9670 (word/sec=1308.14)\n",
            "--finished 5000 sentences (word/sec=1314.84)\n",
            "--finished 10000 sentences (word/sec=1318.21)\n",
            "--finished 15000 sentences (word/sec=1317.04)\n",
            "--finished 20000 sentences (word/sec=1314.79)\n",
            "--finished 25000 sentences (word/sec=1316.63)\n",
            "--finished 30000 sentences (word/sec=1317.33)\n",
            "--finished 35000 sentences (word/sec=1318.36)\n",
            "--finished 40000 sentences (word/sec=1318.86)\n",
            "iter 3: train loss/word=5.3625, ppl=213.2505 (word/sec=1319.19)\n",
            "--finished 5000 sentences (word/sec=1331.38)\n",
            "--finished 10000 sentences (word/sec=1328.80)\n",
            "--finished 15000 sentences (word/sec=1323.64)\n",
            "--finished 20000 sentences (word/sec=1324.54)\n",
            "--finished 25000 sentences (word/sec=1325.02)\n",
            "--finished 30000 sentences (word/sec=1325.31)\n",
            "--finished 35000 sentences (word/sec=1324.47)\n",
            "--finished 40000 sentences (word/sec=1325.10)\n",
            "iter 4: train loss/word=5.2800, ppl=196.3710 (word/sec=1324.98)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2-RYk7_QXuB",
        "colab_type": "text"
      },
      "source": [
        "# 5)- Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOm0Wq1FM9wT",
        "colab_type": "code",
        "outputId": "f7787a48-fcfc-4b37-ae9f-38f23195e885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dev_words, dev_loss = 0, 0.0\n",
        "start = time.time()\n",
        "for sent_id, sent in enumerate(dev):\n",
        "  my_loss = calc_sent_loss(sent)\n",
        "  dev_loss += my_loss.value()\n",
        "  dev_words += len(sent)\n",
        "print(\"iter %r: dev loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), time.time()-start))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 4: dev loss/word=5.6327, ppl=279.4280, time=14.41s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwmInnJOQpdg",
        "colab_type": "text"
      },
      "source": [
        "# 6)-Generate a few sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aIqjrDFQmXI",
        "colab_type": "code",
        "outputId": "7e90ed6d-d397-4fd4-8cc9-c43d16267817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "for _ in range(5):\n",
        "    sent = generate_sent()\n",
        "    print(\" \".join([i2w[x] for x in sent]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "it was <unk> and mr. icahn won the eyes\n",
            "the dollar problem trade with face <unk> ad executive said such managers could come from the takeover below were managed by activities\n",
            "sales of $ N a share a N N and wednesday cut it about the company to N million in the marina <unk> <unk> furs despite stock prices inflation in fees mcdonald d. <unk> in & financing which include <unk> into N\n",
            "time it still late friday crash rose N N of the bonds <unk> book a specific certificates series gained N was priced at a plant in <unk> the company said the revenue french interest payments to which have intended\n",
            "three other money managers largest funds is the allegations that those is did n't release this way can not be a plunged N N here can do not say whether those handling of this was a important part that the <unk> quantum and liability have diseases and <unk> corp\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}