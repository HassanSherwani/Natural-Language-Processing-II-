{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "bow.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFBOqODHw6mV",
        "colab_type": "text"
      },
      "source": [
        "Bag of Word model using PyTorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p-i1DRYTq_i",
        "colab_type": "text"
      },
      "source": [
        "# 1)-Importing key Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9CuXeO1xlV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#support both Python 2 and Python 3 with minimal overhead.\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# I am an engineer. I care only about error not warning. So, let's be maverick and ignore warnings.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WRoiH9wxGPL",
        "colab_type": "code",
        "outputId": "4a1290e2-5c20-41de-9f4f-fda4bd8cec18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "! pip install torch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1+cu100)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiA4L15Iw7kb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu1EDIb5UNq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper file\n",
        "\n",
        "from model import BoW"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcCkkT0mx2Yc",
        "colab_type": "code",
        "outputId": "645590c7-135a-48f9-ae6c-21bab8a7fa69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "! pip install version_information"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting version_information\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b0/6088e15b9ac43a08ccd300d68e0b900a20cf62077596c11ad11dd8cc9e4b/version_information-1.0.3.tar.gz\n",
            "Building wheels for collected packages: version-information\n",
            "  Building wheel for version-information (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for version-information: filename=version_information-1.0.3-cp36-none-any.whl size=3880 sha256=6ccd13c89048addd2da257888b2a768736dced037c6b73730f4956992d9769a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/4c/b3/1976ac11dbd802723b564de1acaa453a72c36c95827e576321\n",
            "Successfully built version-information\n",
            "Installing collected packages: version-information\n",
            "Successfully installed version-information-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z56btwC7x268",
        "colab_type": "code",
        "outputId": "fa418641-9cea-4bc9-b10d-7a811a4c1d13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "# first install: pip install version_information\n",
        "%reload_ext version_information\n",
        "%version_information pandas,torch,numpy"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "\\begin{tabular}{|l|l|}\\hline\n{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\nPython & 3.6.8 64bit [GCC 8.3.0] \\\\ \\hline\nIPython & 5.5.0 \\\\ \\hline\nOS & Linux 4.14.137+ x86\\_64 with Ubuntu 18.04 bionic \\\\ \\hline\npandas & 0.25.3 \\\\ \\hline\ntorch & 1.3.1+cu100 \\\\ \\hline\nnumpy & 1.17.3 \\\\ \\hline\n\\hline \\multicolumn{2}{|l|}{Mon Nov 11 16:35:23 2019 UTC} \\\\ \\hline\n\\end{tabular}\n",
            "application/json": {
              "Software versions": [
                {
                  "version": "3.6.8 64bit [GCC 8.3.0]",
                  "module": "Python"
                },
                {
                  "version": "5.5.0",
                  "module": "IPython"
                },
                {
                  "version": "Linux 4.14.137+ x86_64 with Ubuntu 18.04 bionic",
                  "module": "OS"
                },
                {
                  "version": "0.25.3",
                  "module": "pandas"
                },
                {
                  "version": "1.3.1+cu100",
                  "module": "torch"
                },
                {
                  "version": "1.17.3",
                  "module": "numpy"
                }
              ]
            },
            "text/html": [
              "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.6.8 64bit [GCC 8.3.0]</td></tr><tr><td>IPython</td><td>5.5.0</td></tr><tr><td>OS</td><td>Linux 4.14.137+ x86_64 with Ubuntu 18.04 bionic</td></tr><tr><td>pandas</td><td>0.25.3</td></tr><tr><td>torch</td><td>1.3.1+cu100</td></tr><tr><td>numpy</td><td>1.17.3</td></tr><tr><td colspan='2'>Mon Nov 11 16:35:23 2019 UTC</td></tr></table>"
            ],
            "text/plain": [
              "Software versions\n",
              "Python 3.6.8 64bit [GCC 8.3.0]\n",
              "IPython 5.5.0\n",
              "OS Linux 4.14.137+ x86_64 with Ubuntu 18.04 bionic\n",
              "pandas 0.25.3\n",
              "torch 1.3.1+cu100\n",
              "numpy 1.17.3\n",
              "Mon Nov 11 16:35:23 2019 UTC"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTa40YTaxRXk",
        "colab_type": "text"
      },
      "source": [
        "# 2)- Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO2kSQmrMuQX",
        "colab_type": "code",
        "outputId": "6f357051-e54e-4df5-c501-0b0af0506ae1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df_train = pd.read_fwf('train.txt', sep='|||', names=['sentiment','sep','text','NaN'])\n",
        "df_train.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8544, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_oK4ZiVMuYI",
        "colab_type": "code",
        "outputId": "2bb31e0f-2fd0-4ce5-f9f9-f056c7348afc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>sep</th>\n",
              "      <th>text</th>\n",
              "      <th>NaN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>|||</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>|||</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>|||</td>\n",
              "      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>|||</td>\n",
              "      <td>You 'd think by now America would have had eno...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>|||</td>\n",
              "      <td>Yet the act is still charming here .</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  sep                                               text  NaN\n",
              "0          3  |||  The Rock is destined to be the 21st Century 's...  NaN\n",
              "1          4  |||  The gorgeously elaborate continuation of `` Th...  NaN\n",
              "2          3  |||  Singer\\/composer Bryan Adams contributes a sle...  NaN\n",
              "3          2  |||  You 'd think by now America would have had eno...  NaN\n",
              "4          3  |||               Yet the act is still charming here .  NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBVvY97yMucw",
        "colab_type": "code",
        "outputId": "ced72973-5fd6-4908-f2c8-252ceee8da0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "df_train.sentiment.value_counts()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    2322\n",
              "1    2218\n",
              "2    1624\n",
              "4    1288\n",
              "0    1092\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b2eqfPeNYGr",
        "colab_type": "code",
        "outputId": "56ef917c-cf53-4636-fb19-eabbd7e915a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "df_train.sentiment.value_counts().plot(kind='bar')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f89a50379e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD1CAYAAAC87SVQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nl6RGjL4kNWL0JakRoy9JjfwvZRdB0Dx2UPAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwirF9WxNbAY",
        "colab_type": "text"
      },
      "source": [
        "### loading data using traditional format\n",
        "using open()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XguR09WKw5uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions to read in the corpus\n",
        "w2i = defaultdict(lambda: len(w2i))\n",
        "t2i = defaultdict(lambda: len(t2i))\n",
        "UNK = w2i[\"<unk>\"]\n",
        "def read_dataset(filename):\n",
        "    with open(filename, \"r\") as f:\n",
        "        for line in f:\n",
        "            tag, words = line.lower().strip().split(\" ||| \")\n",
        "            yield ([w2i[x] for x in words.split(\" \")], t2i[tag])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIeUL6YSw5us",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in the data\n",
        "train = list(read_dataset(\"train.txt\"))\n",
        "w2i = defaultdict(lambda: UNK, w2i)\n",
        "dev = list(read_dataset(\"test.txt\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHhdGGgOyMqs",
        "colab_type": "code",
        "outputId": "98e23d98-4a5c-4fac-b7b9-95e6e6e7730e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(len(w2i))\n",
        "print(len(dev))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18648\n",
            "2210\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6imEBuKaxZxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nwords = len(w2i)\n",
        "ntags = len(t2i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVI1anIQVhli",
        "colab_type": "text"
      },
      "source": [
        "# 3)- Initialze Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGroI8b4w5uy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BoW(nwords, ntags)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMUI5g_MVqdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type = torch.LongTensor\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "if use_cuda:\n",
        "    type = torch.cuda.LongTensor\n",
        "    model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFL2bEke1g8K",
        "colab_type": "text"
      },
      "source": [
        "# 4)- Train Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU14n8zgw5u1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f191d763-04bd-44af-df5a-5b9923c838b4"
      },
      "source": [
        "for ITER in range(100):\n",
        "    # Perform training\n",
        "    random.shuffle(train)\n",
        "    train_loss = 0.0\n",
        "    start = time.time()\n",
        "    for words, tag in train:\n",
        "        words = torch.tensor(words).type(type)\n",
        "        tag = torch.tensor([tag]).type(type)\n",
        "        scores = model(words)\n",
        "        loss = criterion(scores, tag)\n",
        "        train_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (\n",
        "                ITER, train_loss/len(train), time.time()-start))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0: train loss/sent=1.4745, time=6.09s\n",
            "iter 1: train loss/sent=1.1216, time=6.01s\n",
            "iter 2: train loss/sent=0.9120, time=6.06s\n",
            "iter 3: train loss/sent=0.7683, time=6.00s\n",
            "iter 4: train loss/sent=0.6635, time=5.98s\n",
            "iter 5: train loss/sent=0.5823, time=6.00s\n",
            "iter 6: train loss/sent=0.5165, time=6.01s\n",
            "iter 7: train loss/sent=0.4634, time=5.98s\n",
            "iter 8: train loss/sent=0.4185, time=6.00s\n",
            "iter 9: train loss/sent=0.3816, time=5.97s\n",
            "iter 10: train loss/sent=0.3490, time=6.03s\n",
            "iter 11: train loss/sent=0.3212, time=6.06s\n",
            "iter 12: train loss/sent=0.2965, time=6.06s\n",
            "iter 13: train loss/sent=0.2743, time=6.02s\n",
            "iter 14: train loss/sent=0.2555, time=5.94s\n",
            "iter 15: train loss/sent=0.2383, time=6.03s\n",
            "iter 16: train loss/sent=0.2219, time=5.96s\n",
            "iter 17: train loss/sent=0.2085, time=5.99s\n",
            "iter 18: train loss/sent=0.1955, time=6.00s\n",
            "iter 19: train loss/sent=0.1841, time=6.09s\n",
            "iter 20: train loss/sent=0.1741, time=5.99s\n",
            "iter 21: train loss/sent=0.1644, time=6.06s\n",
            "iter 22: train loss/sent=0.1554, time=6.18s\n",
            "iter 23: train loss/sent=0.1471, time=6.11s\n",
            "iter 24: train loss/sent=0.1395, time=6.16s\n",
            "iter 25: train loss/sent=0.1326, time=6.23s\n",
            "iter 26: train loss/sent=0.1259, time=6.18s\n",
            "iter 27: train loss/sent=0.1199, time=6.13s\n",
            "iter 28: train loss/sent=0.1141, time=6.10s\n",
            "iter 29: train loss/sent=0.1089, time=6.15s\n",
            "iter 30: train loss/sent=0.1041, time=6.17s\n",
            "iter 31: train loss/sent=0.0993, time=6.12s\n",
            "iter 32: train loss/sent=0.0953, time=6.15s\n",
            "iter 33: train loss/sent=0.0906, time=6.15s\n",
            "iter 34: train loss/sent=0.0867, time=6.10s\n",
            "iter 35: train loss/sent=0.0833, time=6.15s\n",
            "iter 36: train loss/sent=0.0795, time=6.15s\n",
            "iter 37: train loss/sent=0.0762, time=6.22s\n",
            "iter 38: train loss/sent=0.0733, time=6.23s\n",
            "iter 39: train loss/sent=0.0702, time=6.12s\n",
            "iter 40: train loss/sent=0.0674, time=6.19s\n",
            "iter 41: train loss/sent=0.0651, time=6.30s\n",
            "iter 42: train loss/sent=0.0623, time=6.24s\n",
            "iter 43: train loss/sent=0.0601, time=6.23s\n",
            "iter 44: train loss/sent=0.0579, time=6.30s\n",
            "iter 45: train loss/sent=0.0554, time=6.25s\n",
            "iter 46: train loss/sent=0.0535, time=6.47s\n",
            "iter 47: train loss/sent=0.0515, time=6.88s\n",
            "iter 48: train loss/sent=0.0495, time=6.15s\n",
            "iter 49: train loss/sent=0.0481, time=5.95s\n",
            "iter 50: train loss/sent=0.0462, time=6.09s\n",
            "iter 51: train loss/sent=0.0444, time=6.14s\n",
            "iter 52: train loss/sent=0.0429, time=6.02s\n",
            "iter 53: train loss/sent=0.0415, time=5.94s\n",
            "iter 54: train loss/sent=0.0400, time=5.95s\n",
            "iter 55: train loss/sent=0.0389, time=5.99s\n",
            "iter 56: train loss/sent=0.0374, time=5.93s\n",
            "iter 57: train loss/sent=0.0362, time=5.99s\n",
            "iter 58: train loss/sent=0.0350, time=6.04s\n",
            "iter 59: train loss/sent=0.0339, time=5.98s\n",
            "iter 60: train loss/sent=0.0327, time=6.00s\n",
            "iter 61: train loss/sent=0.0319, time=6.08s\n",
            "iter 62: train loss/sent=0.0307, time=6.23s\n",
            "iter 63: train loss/sent=0.0298, time=6.08s\n",
            "iter 64: train loss/sent=0.0289, time=6.09s\n",
            "iter 65: train loss/sent=0.0281, time=6.06s\n",
            "iter 66: train loss/sent=0.0270, time=6.03s\n",
            "iter 67: train loss/sent=0.0262, time=6.00s\n",
            "iter 68: train loss/sent=0.0255, time=6.01s\n",
            "iter 69: train loss/sent=0.0246, time=6.05s\n",
            "iter 70: train loss/sent=0.0240, time=6.00s\n",
            "iter 71: train loss/sent=0.0232, time=6.00s\n",
            "iter 72: train loss/sent=0.0226, time=6.00s\n",
            "iter 73: train loss/sent=0.0217, time=6.06s\n",
            "iter 74: train loss/sent=0.0212, time=6.01s\n",
            "iter 75: train loss/sent=0.0205, time=6.06s\n",
            "iter 76: train loss/sent=0.0200, time=6.08s\n",
            "iter 77: train loss/sent=0.0194, time=6.20s\n",
            "iter 78: train loss/sent=0.0189, time=6.10s\n",
            "iter 79: train loss/sent=0.0183, time=6.03s\n",
            "iter 80: train loss/sent=0.0178, time=6.04s\n",
            "iter 81: train loss/sent=0.0173, time=6.05s\n",
            "iter 82: train loss/sent=0.0168, time=6.04s\n",
            "iter 83: train loss/sent=0.0162, time=6.09s\n",
            "iter 84: train loss/sent=0.0158, time=6.05s\n",
            "iter 85: train loss/sent=0.0153, time=6.05s\n",
            "iter 86: train loss/sent=0.0150, time=6.05s\n",
            "iter 87: train loss/sent=0.0145, time=6.09s\n",
            "iter 88: train loss/sent=0.0141, time=6.11s\n",
            "iter 89: train loss/sent=0.0137, time=6.05s\n",
            "iter 90: train loss/sent=0.0133, time=6.21s\n",
            "iter 91: train loss/sent=0.0131, time=6.15s\n",
            "iter 92: train loss/sent=0.0127, time=6.14s\n",
            "iter 93: train loss/sent=0.0124, time=6.13s\n",
            "iter 94: train loss/sent=0.0121, time=6.07s\n",
            "iter 95: train loss/sent=0.0117, time=6.04s\n",
            "iter 96: train loss/sent=0.0114, time=6.10s\n",
            "iter 97: train loss/sent=0.0113, time=6.49s\n",
            "iter 98: train loss/sent=0.0108, time=6.59s\n",
            "iter 99: train loss/sent=0.0107, time=6.16s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD7kzSxGV7SI",
        "colab_type": "text"
      },
      "source": [
        "# 5)- Test Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZpYXQwUV6kw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "098e8dc8-37de-44a0-cb36-c7b0527c28d1"
      },
      "source": [
        "test_correct = 0.0\n",
        "for words, tag in dev:\n",
        "  words = torch.tensor(words).type(type)\n",
        "  scores = model(words)[0].detach().cpu().numpy()\n",
        "  predict = np.argmax(scores)\n",
        "  if predict == tag:\n",
        "    test_correct += 1\n",
        "print(\"iter %r: test acc=%.4f\" % (ITER, test_correct/len(dev)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 99: test acc=0.3629\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}