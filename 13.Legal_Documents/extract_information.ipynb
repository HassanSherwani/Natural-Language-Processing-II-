{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Extracting information from legal document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1)- Importing key Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#support both Python 2 and Python 3 with minimal overhead.\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "import PyPDF2 as p2\n",
    "import numpy as np \n",
    "import pickle\n",
    "import pandas as pd \n",
    "\n",
    "#For Visuals\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(15,9)})\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 11, 8\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "import nltk  # for text manipulation \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re    # for regular expressions \n",
    "import spacy # another text processing library like nltk\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "3.7.4 64bit [Clang 4.0.1 (tags/RELEASE_401/final)]"
        },
        {
         "module": "IPython",
         "version": "7.8.0"
        },
        {
         "module": "OS",
         "version": "Darwin 19.0.0 x86_64 i386 64bit"
        },
        {
         "module": "pandas",
         "version": "0.25.1"
        },
        {
         "module": "numpy",
         "version": "1.17.2"
        },
        {
         "module": "nltk",
         "version": "3.4.5"
        },
        {
         "module": "seaborn",
         "version": "0.9.0"
        },
        {
         "module": "matplotlib",
         "version": "3.1.1"
        },
        {
         "module": "re",
         "version": "2.2.1"
        },
        {
         "module": "PyPDF2",
         "version": "1.26.0"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.7.4 64bit [Clang 4.0.1 (tags/RELEASE_401/final)]</td></tr><tr><td>IPython</td><td>7.8.0</td></tr><tr><td>OS</td><td>Darwin 19.0.0 x86_64 i386 64bit</td></tr><tr><td>pandas</td><td>0.25.1</td></tr><tr><td>numpy</td><td>1.17.2</td></tr><tr><td>nltk</td><td>3.4.5</td></tr><tr><td>seaborn</td><td>0.9.0</td></tr><tr><td>matplotlib</td><td>3.1.1</td></tr><tr><td>re</td><td>2.2.1</td></tr><tr><td>PyPDF2</td><td>1.26.0</td></tr><tr><td colspan='2'>Wed Dec 18 12:15:46 2019 CET</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 3.7.4 64bit [Clang 4.0.1 (tags/RELEASE\\_401/final)] \\\\ \\hline\n",
       "IPython & 7.8.0 \\\\ \\hline\n",
       "OS & Darwin 19.0.0 x86\\_64 i386 64bit \\\\ \\hline\n",
       "pandas & 0.25.1 \\\\ \\hline\n",
       "numpy & 1.17.2 \\\\ \\hline\n",
       "nltk & 3.4.5 \\\\ \\hline\n",
       "seaborn & 0.9.0 \\\\ \\hline\n",
       "matplotlib & 3.1.1 \\\\ \\hline\n",
       "re & 2.2.1 \\\\ \\hline\n",
       "PyPDF2 & 1.26.0 \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Wed Dec 18 12:15:46 2019 CET} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 3.7.4 64bit [Clang 4.0.1 (tags/RELEASE_401/final)]\n",
       "IPython 7.8.0\n",
       "OS Darwin 19.0.0 x86_64 i386 64bit\n",
       "pandas 0.25.1\n",
       "numpy 1.17.2\n",
       "nltk 3.4.5\n",
       "seaborn 0.9.0\n",
       "matplotlib 3.1.1\n",
       "re 2.2.1\n",
       "PyPDF2 1.26.0\n",
       "Wed Dec 18 12:15:46 2019 CET"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext version_information\n",
    "%version_information pandas,numpy, nltk, seaborn, matplotlib, re, PyPDF2,gensim, spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I personally like Spacy more than nltk. But, I never take sides so, I ll use both where they come handy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2)- Loading data\n",
    "\n",
    "- 1st from pdf to text\n",
    "- 2ndly text to proper data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code 1: for converting generic text from pdf to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract content of given pdf file\n",
    "\n",
    "def getpdf(pdffile):\n",
    "    myfile=p2.PdfFileReader(pdffile)\n",
    "    \n",
    "    # create text file that will contain given pdf 's data\n",
    "    with open('document.txt', 'w') as pdf_output:\n",
    "        for page in range(myfile.getNumPages()):\n",
    "            data= myfile.getPage(page).extractText()\n",
    "            pdf_output.write(data)\n",
    "            \n",
    "            \n",
    "            # read data and cleaning as we have written pdf data into text file\n",
    "            \n",
    "            with open('document.txt' , 'r') as mypdfcontent:\n",
    "                return mypdfcontent.read() # this will return text file and replace all blank spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=getpdf('contract.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code 2: for converting generic text from pdf to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "pdffileobj=open('contract.pdf','rb')\n",
    "pdfreader=PyPDF2.PdfFileReader(pdffileobj)\n",
    "x=pdfreader.numPages\n",
    "pageobj=pdfreader.getPage(x-1)\n",
    "text=pageobj.extractText()\n",
    "\n",
    "file1=open(r\"contract.txt\",\"a\")\n",
    "file1.writelines(text)\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code 3:for converting generic text from pdf to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from io import StringIO\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import os\n",
    "import sys, getopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n#converts pdf, returns its text content as a string\\ndef convert(fname, pages=None):\\n    if not pages:\\n        pagenums = set()\\n    else:\\n        pagenums = set(pages)\\n\\n    output = StringIO()\\n    manager = PDFResourceManager()\\n    converter = TextConverter(manager, output, laparams=LAParams())\\n    interpreter = PDFPageInterpreter(manager, converter)\\n\\n    infile = file(fname, \\'rb\\')\\n    for page in PDFPage.get_pages(infile, pagenums):\\n        interpreter.process_page(page)\\n    infile.close()\\n    converter.close()\\n    text = output.getvalue()\\n    output.close\\n    return text \\n   \\ndef convertMultiple(pdfDir, txtDir):\\n    if pdfDir == \"\": pdfDir = os.getcwd() + \"\\\\\" #if no pdfDir passed in \\n    for pdf in os.listdir(pdfDir): #iterate through pdfs in pdf directory\\n        fileExtension = pdf.split(\".\")[-1]\\n        if fileExtension == \"pdf\":\\n            pdfFilename = pdfDir + pdf \\n            text = convert(pdfFilename) #get string of text content of pdf\\n            textFilename = txtDir + pdf + \".txt\"\\n            textFile = open(textFilename, \"w\") #make text file\\n            textFile.write(text) #write text to text file\\n\\t\\t\\t#textFile.close\\n\\npdfDir = \"/Users/hassansherwani/Documents/paper\"\\ntxtDir = \"/Users/hassansherwani/Documents/paper\"\\nconvertMultiple(pdfDir, txtDir)\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "#converts pdf, returns its text content as a string\n",
    "def convert(fname, pages=None):\n",
    "    if not pages:\n",
    "        pagenums = set()\n",
    "    else:\n",
    "        pagenums = set(pages)\n",
    "\n",
    "    output = StringIO()\n",
    "    manager = PDFResourceManager()\n",
    "    converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "    infile = file(fname, 'rb')\n",
    "    for page in PDFPage.get_pages(infile, pagenums):\n",
    "        interpreter.process_page(page)\n",
    "    infile.close()\n",
    "    converter.close()\n",
    "    text = output.getvalue()\n",
    "    output.close\n",
    "    return text \n",
    "   \n",
    "def convertMultiple(pdfDir, txtDir):\n",
    "    if pdfDir == \"\": pdfDir = os.getcwd() + \"\\\\\" #if no pdfDir passed in \n",
    "    for pdf in os.listdir(pdfDir): #iterate through pdfs in pdf directory\n",
    "        fileExtension = pdf.split(\".\")[-1]\n",
    "        if fileExtension == \"pdf\":\n",
    "            pdfFilename = pdfDir + pdf \n",
    "            text = convert(pdfFilename) #get string of text content of pdf\n",
    "            textFilename = txtDir + pdf + \".txt\"\n",
    "            textFile = open(textFilename, \"w\") #make text file\n",
    "            textFile.write(text) #write text to text file\n",
    "\t\t\t#textFile.close\n",
    "\n",
    "pdfDir = \"/Users/hassansherwani/Documents/paper\"\n",
    "txtDir = \"/Users/hassansherwani/Documents/paper\"\n",
    "convertMultiple(pdfDir, txtDir)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using Pandas for data loading vs open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_table('xxx.txt',names=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>TRADEMARK AND DOMAIN NAME AGREEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>This agreement (the “Agreement”) is by and bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(“Party”) and Eclipse Foundation, Inc. (“Eclip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>201[__] (the “Effective Date”).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>WHEREAS, Party is the owner of certain tradema...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0               TRADEMARK AND DOMAIN NAME AGREEMENT \n",
       "1  This agreement (the “Agreement”) is by and bet...\n",
       "2  (“Party”) and Eclipse Foundation, Inc. (“Eclip...\n",
       "3                   201[__] (the “Effective Date”). \n",
       "4  WHEREAS, Party is the owner of certain tradema..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 TRADEMARK AND DOMAIN NAME AGREEMENT \n",
       "1    This agreement (the “Agreement”) is by and bet...\n",
       "2    (“Party”) and Eclipse Foundation, Inc. (“Eclip...\n",
       "3                     201[__] (the “Effective Date”). \n",
       "4    WHEREAS, Party is the owner of certain tradema...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page 5 of 5 '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[170]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas is more useful if we had more features to work with and it could form a table. Pandas is more for dataframes and here, we have only generic text data with unstructured patterns and there is no point in using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xxx.txt') as f:\n",
    "    docu = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TRADEMARK AND DOMAIN NAME AGREEMENT ',\n",
       " '',\n",
       " 'This agreement (the “Agreement”) is by and between ____________________________ ',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '(“Party”) and Eclipse Foundation, Inc. (“Eclipse”) and is effective as of [______] [_____],  ',\n",
       " '201[__] (the “Effective Date”). ',\n",
       " ' ',\n",
       " '']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 10 lines\n",
    "docu[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3)-Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_stage1=[i.replace('\\xe2\\x80\\x9c','') for i in docu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TRADEMARK AND DOMAIN NAME AGREEMENT ', '', 'This agreement (the “Agreement”) is by and between ____________________________ ', '', ' ', ' ', '(“Party”) and Eclipse Foundation, Inc. (“Eclipse”) and is effective as of [______] [_____],  ', '201[__] (the “Effective Date”). ', ' ', '']\n"
     ]
    }
   ],
   "source": [
    "print(clean_stage1[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean2=[i.replace('\\xe2\\x80\\x9d','') for i in clean_stage1 ]\n",
    "clean3=[i.replace('\\xe2\\x80\\x99s','') for i in clean2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean4 = [x for x in clean3 if x != ' ']\n",
    "clean5 = [x for x in clean4 if x != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clean=[re.sub(\"[^a-zA-Z]+\", \" \", s) for s in clean5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TRADEMARK AND DOMAIN NAME AGREEMENT ',\n",
       " 'This agreement the Agreement is by and between ',\n",
       " ' Party and Eclipse Foundation Inc Eclipse and is effective as of ',\n",
       " ' the Effective Date ',\n",
       " 'WHEREAS Party is the owner of certain trademarks identified in Exhibit A the ',\n",
       " ' Trademarks and of certain domain names identified in Exhibit A the Domain Names ',\n",
       " 'WHEREAS Party is desirous of Eclipse to initiate a project or working group with a ',\n",
       " 'name based on the Trademark the Project Initiation ',\n",
       " 'WHEREAS to accommodate the Project Initiation Party desires to transfer all of Party s ',\n",
       " 'rights title and interest in and to the Trademarks to Eclipse and to thereafter have certain ',\n",
       " 'continuing usage rights of the Trademarks ',\n",
       " 'WHEREAS to accommodate the Project Initiation the Parties may mutually agree to ',\n",
       " 'have any related Domain Names initially redirect to a URL designated by Eclipse and that the ',\n",
       " 'Domain Names are subsequently transferred to Eclipse ',\n",
       " 'In consideration of the foregoing and for other good and valuable consideration the ']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_clean[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines look more aligned in comparision to original document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TRADEMARK AND DOMAIN NAME AGREEMENT ',\n",
       " '',\n",
       " 'This agreement (the “Agreement”) is by and between ____________________________ ',\n",
       " '',\n",
       " ' ',\n",
       " ' ',\n",
       " '(“Party”) and Eclipse Foundation, Inc. (“Eclipse”) and is effective as of [______] [_____],  ',\n",
       " '201[__] (the “Effective Date”). ',\n",
       " ' ',\n",
       " '',\n",
       " 'WHEREAS, Party is the owner of certain trademarks identified in Exhibit A (the ',\n",
       " '',\n",
       " '“Trademarks”) and of certain domain names identified in Exhibit A (the “Domain Names”); ',\n",
       " '',\n",
       " ' ']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docu[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the data\n",
    "\n",
    "using tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contract data has chars 412\n",
      "Contract data has unique chars 173\n"
     ]
    }
   ],
   "source": [
    "chars=list(set(docu))\n",
    "\n",
    "data_size, vocab_size = len(docu), len(chars)\n",
    "print (\"Contract data has chars\", data_size)\n",
    "print (\"Contract data has unique chars\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize Sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove extra characters\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['trademark', 'and', 'domain', 'name', 'agreement']]\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "#data = docu.tolist()\n",
    "data_words = list(sent_to_words(docu))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Bigram, Trigram Models,Postag and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = en_core_web_md.load()\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(data_words)  # processed Text Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_ready = pd.DataFrame(data_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>trademark</td>\n",
       "      <td>domain</td>\n",
       "      <td>name</td>\n",
       "      <td>agreement</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>agreement</td>\n",
       "      <td>agreement</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1     2          3     4     5     6     7     8     9    \\\n",
       "0  trademark     domain  name  agreement  None  None  None  None  None  None   \n",
       "1       None       None  None       None  None  None  None  None  None  None   \n",
       "2  agreement  agreement  None       None  None  None  None  None  None  None   \n",
       "3       None       None  None       None  None  None  None  None  None  None   \n",
       "4       None       None  None       None  None  None  None  None  None  None   \n",
       "\n",
       "   ...   180   181   182   183   184   185   186   187   188   189  \n",
       "0  ...  None  None  None  None  None  None  None  None  None  None  \n",
       "1  ...  None  None  None  None  None  None  None  None  None  None  \n",
       "2  ...  None  None  None  None  None  None  None  None  None  None  \n",
       "3  ...  None  None  None  None  None  None  None  None  None  None  \n",
       "4  ...  None  None  None  None  None  None  None  None  None  None  \n",
       "\n",
       "[5 rows x 190 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_ready.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(412, 190)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_ready.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "df_data_ready.to_pickle('data_ready.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4)- Model Buidling\n",
    "\n",
    "Topic model for keyword extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1)- Create Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of words\n",
    "id2word = corpora.Dictionary(data_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n"
     ]
    }
   ],
   "source": [
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 agreement 24\n",
      "1 domain 15\n",
      "2 name 21\n",
      "3 trademark 30\n",
      "4 effective 9\n",
      "5 date 11\n",
      "6 certain 4\n",
      "7 exhibit 5\n",
      "8 identify 3\n",
      "9 owner 3\n",
      "10 party 32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count = 0\n",
    "for k, v in id2word.iteritems():\n",
    "    print(k, v, id2word.dfs[k])\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we could use a filter but data is very small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2)-Create Corpus: Term Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WHEREAS, Party is the owner of certain trademarks identified in Exhibit A (the '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docu[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['party', 'owner', 'certain', 'trademark', 'identify', 'exhibit']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ready[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "corpus[10] # 10 as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 3 (\"trademark\") appears 1 time.\n",
      "Word 6 (\"certain\") appears 1 time.\n",
      "Word 7 (\"exhibit\") appears 1 time.\n",
      "Word 8 (\"identify\") appears 1 time.\n",
      "Word 9 (\"owner\") appears 1 time.\n",
      "Word 10 (\"party\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_10 = corpus[10]\n",
    "\n",
    "for i in range(len(bow_doc_10)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_10[i][0], \n",
    "                                                     id2word[bow_doc_10[i][0]], \n",
    "                                                     bow_doc_10[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3)- LDA\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "- corpus ({iterable of list of (int, float), scipy.sparse.csc}, optional) – Stream of document vectors or sparse matrix of shape (num_terms, num_documents). If not given, the model is left untrained (presumably because you want to call update() manually).\n",
    "- num_topics (int, optional) – The number of requested latent topics to be extracted from the training corpus.\n",
    "- id2word ({dict of (int, str), gensim.corpora.dictionary.Dictionary}) – Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
    "- distributed (bool, optional) – Whether distributed computing should be used to accelerate training.\n",
    "- chunksize (int, optional) – Number of documents to be used in each training chunk.\n",
    "- passes (int, optional) – Number of passes through the corpus during training.\n",
    "- update_every (int, optional) – Number of documents to be iterated through for each update. Set to 0 for batch learning, > 1 for online iterative learning.\n",
    "- alpha ({numpy.ndarray, str}, optional) – Can be set to an 1D array of length equal to the number of expected topics that expresses our a-priori belief for the each topics’ probability. Alternatively default prior selecting strategies can be employed by supplying a string:\n",
    "\n",
    "’asymmetric’: Uses a fixed normalized asymmetric prior of 1.0 / topicno. ’auto’: Learns an asymmetric prior from the corpus (not available if distributed==True).\n",
    "\n",
    "eta ({float, np.array, str}, optional) –\n",
    "\n",
    "A-priori belief on word probability, this can be:\n",
    "\n",
    "scalar for a symmetric prior over topic/word probability, vector of length num_words to denote an asymmetric user defined probability for each word, matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination, the string ‘auto’ to learn the asymmetric prior from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus=corpus, num_topics=5, id2word=id2word, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.save('topic_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "lda_model = LdaModel.load('topic_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.071*\"trademark\" + 0.062*\"party\" + 0.027*\"eclipse\" + 0.027*\"title\" + '\n",
      "  '0.021*\"assignment\" + 0.019*\"name\" + 0.019*\"transfer\" + 0.019*\"date\" + '\n",
      "  '0.018*\"domain\" + 0.018*\"interest\"'),\n",
      " (1,\n",
      "  '0.078*\"agreement\" + 0.041*\"party\" + 0.027*\"page\" + 0.024*\"write\" + '\n",
      "  '0.020*\"eclipse\" + 0.017*\"waiver\" + 0.014*\"name\" + 0.012*\"independent\" + '\n",
      "  '0.012*\"contractor\" + 0.011*\"term\"'),\n",
      " (2,\n",
      "  '0.034*\"trademark\" + 0.028*\"provision\" + 0.023*\"delivery\" + 0.022*\"material\" '\n",
      "  '+ 0.022*\"document\" + 0.021*\"notice\" + 0.021*\"execution\" + 0.013*\"guideline\" '\n",
      "  '+ 0.012*\"arise\" + 0.012*\"demand\"'),\n",
      " (3,\n",
      "  '0.055*\"agreement\" + 0.034*\"party\" + 0.032*\"term\" + 0.031*\"condition\" + '\n",
      "  '0.029*\"waiver\" + 0.016*\"interest\" + 0.016*\"require\" + 0.015*\"write\" + '\n",
      "  '0.014*\"title\" + 0.014*\"law\"'),\n",
      " (4,\n",
      "  '0.069*\"name\" + 0.057*\"domain\" + 0.037*\"party\" + 0.031*\"trademark\" + '\n",
      "  '0.026*\"remedy\" + 0.018*\"include\" + 0.017*\"date\" + 0.017*\"transfer\" + '\n",
      "  '0.017*\"effective\" + 0.014*\"agreement\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "#The trained topics (keywords and weights)\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5)-Evaluating model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1)- Perplexity\n",
    "\n",
    "Lower is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -5.805305456585416\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity:a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2)-Coherence Score\n",
    "\n",
    "Higher is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.5263038663720127\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_ready, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3- Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis, 'topics_LDApyvis.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
